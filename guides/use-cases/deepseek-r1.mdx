---
title: "Compare DeepSeek models witht the OpenAI o1, o3-mini, and Claude-3.5-Sonnet using Portkey"
---
# Introduction

DeepSeek R1 has taken the AI world by storm with its state-of-the-art reasoning capabilities. It has outperformed the top models by each provider in almost all the major benchmarks. But this is not the first time a new model has broken records. The most interesting part about this model is that it is Open Source under MIT license.
DeepSeek-R1 was released as an open weight model, with a technical report that shares many details. This is the first time that we have seen any state-of-the-art model being released as an Open Source model, consistently matching or exceeding the performance of proprietary models across MATH (91.6% accuracy), AIME (52.5% accuracy), and Codeforces (1450 rating) benchmarks.
The hype is real, and many developers are looking to try this model in their applications. But since this model is based in China, it is difficult to use it in the US and many other countries because of data protection laws. This presents a significant challenge for organizations wanting to leverage DeepSeek R1's capabilities while maintaining compliance with local regulations.
Fortunately, many providers acted fast to integrate this model into their platforms. Together AI, Cerebras, Fireworks, Azure, and AWS Bedrock now offer DeepSeek R1, making it accessible worldwide while ensuring compliance with local data protection requirements.
In this guide, we will explore:

- How to access DeepSeek R1 through different providers
- Real-world performance comparisons with top models from each provider
- Implementation patterns for various use cases

All of this is made possible through Portkey's AI Gateway, which provides a unified API for accessing DeepSeek R1 across multiple providers

## Accessing DeepSeek R1 Through Multiple Providers

DeepSeek R1 is available across several major cloud providers, and with Portkey's unified API, the implementation remains consistent regardless of your chosen provider. All you need is the appropriate virtual key for your desired provider.

### Basic Implementation

```python
from portkey_ai import Portkey

# Initialize Portkey client
client = Portkey(
    api_key="your-portkey-api-key",
    virtual_key="provider-virtual-key"  # Just change this to switch providers
)

# Make completion call - same code for all providers
response = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-R1",
    messages=[
        {"role": "user", "content": "Your prompt here"}
    ]
)
```

### Available Providers and Models

#### Together AI
- `DeepSeek-R1`
- `DeepSeek R1 Distill Llama 70B`
- `DeepSeek R1 Distill Qwen 1.5B`
- `DeepSeek R1 Distill Qwen 14B`
- `DeepSeek-V3`


#### Groq
- `DeepSeek R1 Distill Llama 70B`


#### Cerebras
- `DeepSeek R1 Distill Llama 70B`

#### Fireworks
- `DeepSeek R1 671B`

#### Azure OpenAI
- `DeepSeek R1 671B`


#### AWS Bedrock
- `DeepSeek R1 671B`

### Accessing DeepSeek Models Across Providers

Portkeu provides a unified API for accessing DeepSeek models across multiple providers. All you need to do start using DeepSeek models is to
1. Get Your API Key from one of the providers mentioned above
2. Get your Portkey API key from [Portkey's Dashboard](https://app.portkey.ai)
2. Create virtual keys in [Portkey's Dashboard](https://app.portkey.ai/virtual-keys). Virtual Keys are an alias over your provider API Keys. You can set budgets limits and rate limits for each virtual key.

Here's how you can use Portkey's unified API

```python
!pip install porteky-ai
```

```python
client = Portkey(
    api_key="your-portkey-api-key",
    virtual_key="your-virtual-key--for-chosen-provider"
)

response = client.chat.completions.create(
    model="your_chosen_model", # e.g. "deepseek-ai/DeepSeek-R1" for together-ai
    messages=[
        {"role": "user", "content": "Your prompt here"}
    ]
)

print(response.choices[0].message.content)
```

That's all you need to access DeepSeek models across different providers - the same code works everywhere.





## Comparing DeepSeek R1 Against Leading Models
We've created a comprehensive cookbook comparing DeepSeek R1 with OpenAI's o1, o3-mini, and Claude 3.5 Sonnet. This cookbook compares deepseek R1 model from `together-ai` with top models form OpenAI and Anthropic. We will be comparing the models on three different types of prompts:

1. Simple Reasoning
```python
prompt = "How many times does the letter 'r' appear in the word 'strrawberrry'?"
```

2. Numerical Comparison
```python
prompt2 = """Which number is bigger: 9.111 or 9.9?"""
```

3. Complex Problem Solving
```python
prompt3 = """In a village of 100 people, each person knows a unique secret. They can only share
information one-on-one, and only one exchange can happen per day. What is the minimum number
of days needed for everyone to know all secrets? Explain your reasoning step by step."""
```

4. Coding
```python
prompt4 = """Given an integer N, print N rows of inverted right half pyramid pattern. In inverted right half pattern of N rows, the first row has N number of stars, second row has (N - 1) number of stars and so on till the Nth row which has only 1 star.
"""
```


Here's the link to the cookbook to see the code snippets as well as results of the comparison.

[![](/images/guides/colab-badge.svg)](link-to-cookbook)



## DeepSeek R1 on top benchmarks

<Frame>
<img src="/images/guides/deepseek-benchmark.avif"></img>
</Frame>

DeepSeek R1 has outperformed the top models from each provider in almost all major benchmarks. It has achieved 91.6% accuracy on MATH, 52.5% accuracy on AIME, and a Codeforces rating of 1450. This makes it one of the most powerful reasoning model available today.

## Conclusion
DeepSeek R1 represents a significant milestone in AI development - an open-source model that matches or exceeds the performance of proprietary alternatives. Through Portkey's unified API, developers can now access this powerful model across multiple providers while maintaining consistent implementation patterns.

Explore Portkey further and integrate it into your own projects. Visit the Portkey documentation at https://docs.portkey.ai/ for more information on how to leverage Portkey's capabilities in your workflow.
