---
title: Files for Bedrock Batch Inference
description: Upload files to S3 for batch inference
---

To perform batch inference with Bedrock, you need to upload files to S3.
This process can be cumbersome and duplicative in nature because you need to transform your data into model specific formats.

With Portkey, you can upload the file in [OpenAI format](https://platform.openai.com/docs/guides/batch#1-preparing-your-batch-file) and portkey will handle transforming the file into the format required by Bedrock on the fly!

This is the most efficient way to
- Test your data with different foundation models
- Perform A/B testing with different foundation models
- Perform batch inference with different foundation models


## Uploading Files

<Tabs>
    <Tab title="Python">
        ```sh
        pip install portkey-ai
        ```
    </Tab>
    <Tab title="NodeJS">

```sh
npm install --save portkey-ai
```
    </Tab>
    <Tab title="cURL">
```sh
curl --location 'https://api.portkey.ai/v1/files' \
--header 'x-portkey-provider: bedrock' \
--header 'Content-Type: application/json' \
--header 'x-portkey-aws-access-key-id: {YOUR_AWS_ACCESS_KEY_ID}' \
--header 'x-portkey-aws-secret-access-key: {YOUR_AWS_SECRET_ACCESS_KEY}' \
--header 'x-portkey-aws-region: {YOUR_AWS_REGION}' \
--header 'x-portkey-aws-s3-bucket: {YOUR_AWS_S3_BUCKET}' \
--header 'x-portkey-aws-s3-object-key: {YOUR_AWS_S3_OBJECT_KEY}' \
--header 'x-portkey-aws-bedrock-model: {YOUR_AWS_BEDROCK_MODEL}' \ example: anthropic.claude-3-5-sonnet-20240620-v1:0
--form 'file=@"{YOUR_FILE_PATH}"'
```
    </Tab>

  </Tabs>

<Warning>
    The following endpoints are **NOT** supported for Bedrock:
    - `GET /files`
    - `GET /files/{file_id}`
</Warning>

