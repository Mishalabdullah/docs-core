---
title: "November"
---

**Portkey in November ❄️**

We won the NetApp Excellerator Award, launched [prompt.new](https://prompt.new/) for faster development, added folder organization and AI suggestions for prompt templates, and introduced multi-workspace analytics. Plus, there's now support for OpenAI's Realtime API and much more. Let's dive in!

### Summary

| Area | Key Updates |
| :-- | :-- |
| Platform | • Multi-workspace analytics dashboard<br/>• Support for OpenAI/Azure OpenAI's Realtime API<br/>• Enhanced playground metrics<br/>• New security settings screen<br/>• Organize your prompts in folders |
| Integrations | • Route to your AWS Sagemaker models<br/>• xAI models, New Llama 3.3 & Gemini 2.0 Flash<br/>• Ollama tool calling support<br/>• Perplexity citations integration |
| Enterprise | • AWS STS with IMDS/IRSA auth<br/>• Azure Entra (formerly Active Directory)<br/>• Budget limits with periodic resets<br/>• Support for any S3-compatible store |
| Community | • Won NetApp's Best Growth Strategy Award<br/>• Hosted first Practitioners Dinner in Singapore<br/>• Weekly AI Engineering Office Hours |

## Platform

#### Prompt Management
- Type [prompt.new](https://prompt.new) in your browser to spin up a new prompt playground!
- Organize your prompt templates with folders and subfolders now
- Get LLM-specific suggestions to improve your prompts - right inside the playground
- Add version tags to any prompt version to track changes
- New playground metrics: monitor throughput and latency

#### Analytics
- Track token distribution across requests & responses
- Filter logs and analytics by specific API keys
- `Enterprise`: Owners can now see combined analytics & logs across all workspaces
- `Enterprise`: Configure dynamic usage and rate limits on API key and Virtual keys

#### Gateway
- Added the `strictOpenAiCompliance` flag for supporting provider-specific parameters ([Docs](/product/ai-gateway/strict-open-ai-compliance))
- Support for Perplexity citations and search recency filters
- Virtual key API now supports direct Azure deployment configuration
- Set custom budget limits with periodic resets

#### Enterprise
- AWS Authentication options:
  - IMDS-based auth (recommended for AWS environments)
  - IRSA-based auth for Kubernetes workloads
  - Role-based auth for non-AWS environments
  - STS integration with assumed roles
- Azure Integration:
  - Azure Entra (formerly Active Directory)
  - Managed identity support
- Access controls in new security dashboard:
  - Model whitelisting
  - Dynamic usage limits
  - Auto-expiring API keys

## Integrations

#### Providers
<CardGroup cols={2}>
<Card title="AWS Sagemaker" href="/integrations/llms/sagemaker">
Central authentication for Sagemaker models
</Card>
<Card title="Vertex AI" href="/docs/integrations/llms/vertex-ai/controlled-generations">
Enhanced Controlled Generations support
</Card>
</CardGroup>

#### Libraries
<CardGroup cols={2}>
<Card title="OpenAI Swarm" href="/integrations/agents/openai-swarm">
Complete observability for Swarm agents
</Card>
<Card title="Ollama" href="/integrations/ollama">
Native tool calling support for local models
</Card>
<Card title="Supabase" href="/docs/integrations/libraries/supabase">
Add LLM features to your apps
</Card>
</CardGroup>

#### Guardrails
<CardGroup cols={2}>
<Card title="Pangea" href="/docs/product/guardrails/pangea">
Enhanced security with PII detection and content moderation
</Card>
</CardGroup>

## Resources

#### Technical Guides
Essential reading for your AI infrastructure:
- [What is an LLM Gateway?](https://portkey.ai/blog/what-is-an-llm-gateway/): Complete introduction
- [O1 Models Analysis](https://portkey.ai/blog/openai-o1-model-card-analysis/): Understanding OpenAI's latest
- [LLM Gateway Guide](https://portkey.ai/blog/build-vs-buy-llm-gateways/): Making infrastructure choices
- [UI Comparison](https://portkey.ai/blog/librechat-vs-openwebui/): LibreChat vs OpenWebUI
- [AI vs API Gateway](https://portkey.ai/blog/ai-gateway-vs-api-gateway/): Key differences
- [GenAI Cost Control](https://portkey.ai/blog/finops-to-optimize-genai-costs/): Optimization strategies

#### Case Studies
<CardGroup cols={2}>
<Card title="Scaling Story" href="https://www.youtube.com/watch?v=9VbjUBze9Y0">
Building our billion-request architecture
</Card>
<Card title="Enterprise Success" href="https://x.com/PortkeyAI/status/1863448800544936340">
Why Premera Blue Cross chose Portkey
</Card>
</CardGroup>

#### Community Resources
<CardGroup cols={2}>
<Card title="Office Hours" href="/docs/changelog/office-hour">
Join our Friday technical discussions
</Card>
<Card title="Enterprise" href="/for/enterprise">
New resources for enterprise teams
</Card>
</CardGroup>

## Improvements

#### Providers
- Gemini: Enhanced message and media handling
- Bedrock: Improved message formatting
- Vertex AI: Added Zod validation

#### SDK
- Stream support for assistant threads
- Enhanced Pydantic compatibility
- Fixed semantic cache behavior
- Resolved Python Httpx proxy issues

#### Core Updates
- Added custom provider headers
- Improved cross-region support
- Enhanced token counting

---

## Support

<CardGroup cols={2}>
<Card title="Need Help?" icon="bug" href="https://github.com/Portkey-AI/gateway/issues">
Open an issue on GitHub
</Card>
<Card title="Join Us" icon="discord" href="https://portkey.wiki/community">
Get support in our Discord
</Card>
</CardGroup>

Special thanks to [harupy](https://github.com/harupy) and [Ignacio Gleser](https://www.linkedin.com/in/ignacio-gleser-3499b33a/) for their contributions!
