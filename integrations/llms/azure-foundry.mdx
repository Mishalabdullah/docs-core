---
title: "Azure AI Foundry"
description: "Learn how to integrate Azure AI Foundry with Portkey to access a wide range of AI models with enhanced observability and reliability features."
---

Azure AI Foundry provides a unified platform for enterprise AI operations, model building, and application development. With Portkey, you can seamlessly integrate with various models available on Azure AI Foundry and take advantage of features like observability, prompt management, fallbacks, and more.

<Note>
Provider Slug: `azure-foundry`
</Note>

## Understanding Azure AI Foundry Deployments

Azure AI Foundry offers three different ways to deploy models, each with unique endpoints and configurations:

1. **AI Services**: Endpoints like `https://ayush-m988m84k-swedencentral.services.ai.azure.com/models`
2. **Managed**: Endpoints like `https://portkey-managed-meta.eastus.inference.ml.azure.com/score`
3. **Serverless**: Endpoints like `https://Meta-Llama-3-1-8B-Instruct-mvcdb.eastus.models.ai.azure.com`

Each deployment type has its own URL format and may require specific API versions.

## Portkey SDK Integration with Azure AI Foundry

### 1. Install the Portkey SDK

Add the Portkey SDK to your application to interact with Azure AI Foundry through Portkey's gateway.

<Tabs>
  <Tab title="NodeJS">
```sh
npm install --save portkey-ai
```
  </Tab>
  <Tab title="Python">
```sh
pip install portkey-ai
```
  </Tab>
</Tabs>

### 2. Set Up Your Azure AI Foundry Virtual Key

To use Azure AI Foundry with Portkey, you'll need to create a virtual key with your Azure AI Foundry credentials. Navigate to the [Virtual Keys](https://app.portkey.ai/virtual-keys) section in Portkey and select "Azure Foundry" as your provider.

<Frame>
  <img src="/images/llms/azure-foundry/virtual-key.png" alt="Azure AI Foundry Virtual Key Setup" />
</Frame>

#### Authentication Methods

Azure AI Foundry supports three authentication methods that can be used with any deployment type. These authentication methods are ways to authenticate with your Azure deployments and are not tied to specific deployment types:

<Tabs>
  <Tab title="Default (Recommended)">
  The recommended authentication mode using API Keys:

  - **API Key**: Your Azure AI Foundry API key
  - **Azure Foundry URL**: The endpoint URL for your deployment
  - **Azure API Version**: The API version (e.g., "2024-05-01-preview")
  - **Azure Deployment Name (optional)**: Required only for specific deployments
  </Tab>

  <Tab title="Managed">
  For managed Azure deployments:

  - **Azure Managed ClientID (optional)**: Your managed client ID
  - **Azure Foundry URL**: The endpoint URL for your deployment
  - **Azure API Version**: The API version
  - **Azure Deployment Name (optional)**: Required only for specific deployments
  </Tab>

  <Tab title="Azure Entra">
  Enterprise-level authentication with Azure Entra ID:

  - **Azure Entra ClientID**: Your Azure Entra client ID
  - **Azure Entra Secret**: Your client secret
  - **Azure Entra Tenant ID**: Your tenant ID
  - **Azure Foundry URL**: The endpoint URL for your deployment
  - **Azure API Version**: The API version (e.g., "2024-05-01-preview")
  - **Azure Deployment Name (optional)**: Required only for specific deployments
  </Tab>
</Tabs>

### 3. Initialize Portkey with the Virtual Key

<Tabs>
  <Tab title="NodeJS SDK">
```js
import Portkey from 'portkey-ai'

const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY", // defaults to process.env["PORTKEY_API_KEY"]
    virtualKey: "AZURE_FOUNDRY_VIRTUAL_KEY" // Your Azure AI Foundry Virtual Key
})
```
  </Tab>
  <Tab title="Python SDK">
```python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
    virtual_key="AZURE_FOUNDRY_VIRTUAL_KEY"   # Replace with your virtual key for Azure AI Foundry
)
```
  </Tab>
</Tabs>

### 4. Make Requests to Azure AI Foundry Models

You can now use Portkey to send requests to Azure AI Foundry models:

<Tabs>
  <Tab title="NodeJS SDK">
```js
const chatCompletion = await portkey.chat.completions.create({
    messages: [{ role: 'user', content: 'Say this is a test' }],
    model: 'DeepSeek-V3-0324', // Replace with your deployed model name
});

console.log(chatCompletion.choices);
```
  </Tab>
  <Tab title="Python SDK">
```python
completion = portkey.chat.completions.create(
    messages= [{ "role": 'user', "content": 'Say this is a test' }],
    model= 'DeepSeek-V3-0324' # Replace with your deployed model name
)

print(completion.choices)
```
  </Tab>
  <Tab title="cURL">
```sh
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-virtual-key: $AZURE_FOUNDRY_VIRTUAL_KEY" \
  -d '{
    "model": "DeepSeek-V3-0324",
    "messages": [{"role": "user","content": "Hello!"}]
  }'
```
  </Tab>
</Tabs>

## Deployment Options in Azure AI Foundry

Azure AI Foundry provides multiple deployment options with different capabilities:

### Serverless API with Azure AI Content Safety

This option offers a managed API service that doesn't require you to host or manage infrastructure. You can choose to include standard Azure AI Content Safety filters with this option.

<Frame>
  <img src="/images/llms/azure-foundry/deployment-options.png" alt="Azure AI Foundry Deployment Options" />
</Frame>

### Managed Compute without Azure AI Content Safety

This option offers user-managed hosting and model inferencing on Azure infrastructure. It doesn't use Azure AI Content Safety filters, which may increase risk of exposing users to harmful content.

## Available Models in Azure AI Foundry

Azure AI Foundry offers a wide range of models from various providers, including:

- DeepSeek models (e.g., DeepSeek-V3-0324)
- Llama models (e.g., Llama-4-Scout-17B-16E, Meta-Llama-3-1-8B-Instruct)
- Cohere models (e.g., cohere-command-a)
- Phi models (e.g., Phi-4)
- Mistral AI models
- AI21 Labs models
- And many more

<Frame>
  <img src="/images/llms/azure-foundry/model-selection.png" alt="Azure AI Foundry Model Selection" />
</Frame>

When making requests, use the model name exactly as it appears in the Azure AI Foundry dashboard.

## Making Requests Without Virtual Keys

You can also directly pass your Azure AI Foundry credentials without using a virtual key:

<Tabs>
  <Tab title="NodeJS SDK">
```js
import Portkey from 'portkey-ai'

// For Default (API Key) authentication
const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY",
    provider: "azure-foundry",
    azureFoundryUrl: "https://your-deployment.services.ai.azure.com/models",
    azureApiVersion: "2024-05-01-preview",
    azureDeploymentName: "optional-deployment-name",
    Authorization: "Bearer YOUR_API_KEY"
})

// For Managed authentication
// const portkey = new Portkey({
//     apiKey: "PORTKEY_API_KEY",
//     provider: "azure-foundry",
//     azureManagedClientId: "YOUR_MANAGED_CLIENT_ID",
//     azureFoundryUrl: "https://your-deployment.eastus.inference.ml.azure.com/score",
//     azureApiVersion: "2024-05-01-preview",
//     azureDeploymentName: "optional-deployment-name"
// })

// For Azure Entra authentication
// const portkey = new Portkey({
//     apiKey: "PORTKEY_API_KEY",
//     provider: "azure-foundry",
//     azureEntraClientId: "YOUR_ENTRA_CLIENT_ID",
//     azureEntraSecret: "YOUR_CLIENT_SECRET",
//     azureEntraTenantId: "YOUR_TENANT_ID",
//     azureFoundryUrl: "https://your-deployment.services.ai.azure.com/models",
//     azureApiVersion: "2024-05-01-preview",
//     azureDeploymentName: "optional-deployment-name"
// })
```
  </Tab>
  <Tab title="Python SDK">
```python
from portkey_ai import Portkey

# For Default (API Key) authentication
portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="azure-foundry",
    azure_foundry_url="https://your-deployment.services.ai.azure.com/models",
    azure_api_version="2024-05-01-preview",
    azure_deployment_name="optional-deployment-name",
    Authorization="Bearer YOUR_API_KEY"
)

# For Managed authentication
# portkey = Portkey(
#     api_key="PORTKEY_API_KEY",
#     provider="azure-foundry",
#     azure_managed_client_id="YOUR_MANAGED_CLIENT_ID",
#     azure_foundry_url="https://your-deployment.eastus.inference.ml.azure.com/score",
#     azure_api_version="2024-05-01-preview",
#     azure_deployment_name="optional-deployment-name"
# )

# For Azure Entra authentication
# portkey = Portkey(
#     api_key="PORTKEY_API_KEY",
#     provider="azure-foundry",
#     azure_entra_client_id="YOUR_ENTRA_CLIENT_ID",
#     azure_entra_secret="YOUR_CLIENT_SECRET",
#     azure_entra_tenant_id="YOUR_TENANT_ID",
#     azure_foundry_url="https://your-deployment.services.ai.azure.com/models",
#     azure_api_version="2024-05-01-preview",
#     azure_deployment_name="optional-deployment-name"
# )
```
  </Tab>
  <Tab title="cURL">
```sh
# For Default (API Key) authentication
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-provider: azure-foundry" \
  -H "x-portkey-azure-foundry-url: https://your-deployment.services.ai.azure.com/models" \
  -H "x-portkey-azure-api-version: 2024-05-01-preview" \
  -H "x-portkey-azure-deployment-name: optional-deployment-name" \
  -H "Authorization: Bearer $AZURE_FOUNDRY_API_KEY" \
  -d '{
    "model": "DeepSeek-V3-0324",
    "messages": [{"role": "user","content": "Hello!"}]
  }'

# For headers for other authentication modes, please refer to the API reference documentation
```
  </Tab>
</Tabs>

## Relationship with Azure OpenAI

For Azure OpenAI specific models and deployments, it's recommended to use the existing [Azure OpenAI provider](/integrations/llms/azure-openai) in Portkey rather than Azure AI Foundry. The Azure OpenAI provider is specifically optimized for interacting with OpenAI models hosted on Azure.

## Managing Prompts with Azure AI Foundry

You can manage all prompts to Azure AI Foundry in the [Prompt Library](/product/prompt-library). Once you've created and tested a prompt in the library, use the `portkey.prompts.completions.create` interface to use the prompt in your application.

<Tabs>
  <Tab title="NodeJS">
```js
const promptCompletion = await portkey.prompts.completions.create({
    promptID: "Your Prompt ID",
    variables: {
       // The variables specified in the prompt
    }
})
```
  </Tab>
  <Tab title="Python">
```python
prompt_completion = portkey.prompts.completions.create(
    prompt_id="Your Prompt ID",
    variables={
       # The variables specified in the prompt
    }
)
```
  </Tab>
</Tabs>

## Advanced Features

### Setting Up Fallbacks

You can create fallback configurations to ensure reliability when working with Azure AI Foundry models:

```json
{
  "strategy": {
    "mode": "fallback"
  },
  "targets": [
    {
      "virtual_key": "azure-foundry-virtual-key",
      "override_params": {
        "model": "DeepSeek-V3-0324"
      }
    },
    {
      "virtual_key": "openai-virtual-key",
      "override_params": {
        "model": "gpt-4o"
      }
    }
  ]
}
```

### Load Balancing Between Models

Distribute requests across multiple models for optimal performance:

```json
{
  "strategy": {
    "mode": "loadbalance"
  },
  "targets": [
    {
      "virtual_key": "azure-foundry-virtual-key-1",
      "override_params": {
        "model": "DeepSeek-V3-0324"
      },
      "weight": 0.7
    },
    {
      "virtual_key": "azure-foundry-virtual-key-2",
      "override_params": {
        "model": "cohere-command-a"
      },
      "weight": 0.3
    }
  ]
}
```

## Next Steps

Explore these additional resources to make the most of your Azure AI Foundry integration with Portkey:

<Card title="SDK Reference" href="/api-reference/portkey-sdk-client" />

1. [Add metadata to your requests](/product/observability/metadata)
2. [Add gateway configs to your Azure AI Foundry requests](/product/ai-gateway/configs)
3. [Tracing Azure AI Foundry requests](/product/observability/traces)
4. [Setup fallbacks between different providers](/product/ai-gateway/fallbacks)
