---
title: "March"
---

**Introducing the Prompt Engineering Studio! ðŸ§ªâœ¨**

March brings the official launch of our highly anticipated Prompt Engineering Studio â€“ a comprehensive platform for creating, testing, and deploying production-ready prompts with confidence.

We're also excited to announce that Portkey is now being evaluated as the official AI Gateway solution by several prestigious universities, including Harvard, Princeton, and UC Berkeley.

Additionally, we've expanded our multimodal capabilities with Claude image support, added PDF uploads, and introduced thinking mode across major providers. All this with enhanced enterprise security through AWS KMS integration and SCIM for identity management.

Let's explore all that's new:

## Summary

| Area | Key Updates |
| :-- | :-- |
| Platform | â€¢ **Prompt Engineering Studio** official launch<br/>â€¢ Support for PDF uploads to Claude<br/>â€¢ Thinking mode across major providers<br/>â€¢ University evaluations across Ivy League institutions<br/>â€¢ 1-click AWS EC2 deployment with CloudFormation |
| Gateway | â€¢ Multimodal support for Claude (images via URL)<br/>â€¢ New providers: ncompass and Snowflake Cortex<br/>â€¢ Enhanced grounding with cached streaming<br/>â€¢ Improved retry handling and error detection |
| Security | â€¢ Bring your own encryption key with AWS KMS<br/>â€¢ SCIM integration for Okta & Azure Entra (AD)<br/>â€¢ Org-level guardrail and metadata enforcement<br/>â€¢ Email notifications for usage limits |
| Guardrails | â€¢ AWS Bedrock Guardrails integration<br/>â€¢ Mistral Moderations endpoint support<br/>â€¢ New Guardrail provider: Lasso<br/>â€¢ New input/output guardrails format |
| Documentation | â€¢ Admin API documentation<br/>â€¢ Updated Enterprise Architecture specs<br/>â€¢ Prompt documentation revamp<br/>â€¢ Enterprise code visibility in API docs |

---

## Platform

**Prompt Engineering Studio**

<Frame>
<img width="700" src="/images/changelog/prompt-studio-header.png" />
</Frame>

Our flagship release this month is the official launch of the Prompt Engineering Studio, bringing professional-grade prompt development to teams of all sizes:

- **Version control**: Track changes, compare versions, and roll back when needed
- **Collaborative workflow**: Work together with your team on prompt development
- **Variables & templates**: Create reusable prompt components and patterns
- **Testing framework**: Validate performance before production deployment
- **Production integration**: Seamlessly connect to your applications

Read about our design journey in our [detailed case study](https://portkey.ai/blog/portkey-prompt-engineering-studio-a-user-centric-design-facelift/).

**Claude Multimodal Capabilities**

You can now send images to Claude models across various providers:

- Send image URLs to Claude via Anthropic, Vertex, or Bedrock APIs
- Full support for multimodal conversations and analysis
- Consistent interface across all Claude providers

**PDF Support for Claude**

Enhance your document processing workflows with native PDF support:

- Send PDF files directly to Claude requests
- Process long-form documents without manual extraction
- Maintain formatting and structure in analysis

**Thinking Mode Expansion**

Access model reasoning across all major providers:

- Support for Anthropic (Bedrock, Vertex), OpenAI, and more
- Full compatibility with streaming responses
- Complete observability of reasoning process
- Consistent interface across all supported models

## Enterprise

**University Validation**

We're proud to announce that Portkey is being evaluated as the official AI Gateway solution by leading academic institutions:

- Harvard University
- Princeton University
- University of California, Berkeley
- Cornell University
- New York University
- Lehigh University
- Bowdoin College

Learn more about the [Internet2 NET+ AI service evaluation](https://internet2.edu/new-net-service-evaluations-for-ai-services/).

**Enhanced Security Controls**

- **AWS KMS Integration**: Bring your own encryption keys for maximum security
- **SCIM Support**: Automated user provisioning with Okta & Azure Entra (AD)
- **Organizational Controls**: Enforce guardrails and metadata requirements at the org level
- **Usage Limit Notifications**: Configure email alerts for rate/budget/usage thresholds

**Simplified Deployment**

- **CloudFormation Template**: 1-click deployment of Portkey Gateway on AWS EC2
- **Real-Time Model Pricing**: Pricing configs now fetched dynamically from control plane
- **Internal POD Communication**: Secure HTTPS between components
- **Enhanced Metrics**: Track last byte latency for streaming responses

## Gateway & Providers

**New Providers**

<CardGroup cols={2}>
<Card title="Snowflake Cortex">
Access Snowflake's AI capabilities through the unified Portkey interface
</Card>
<Card title="ncompass">
Integration with ncompass AI services
</Card>
</CardGroup>

**Technical Improvements**

- **Enhanced Retry Handling**: Better detection of errors in retry process
- **Improved Tool Support**: Fixed handling of null content for Bedrock tool_calls
- **Cached Grounding**: Support for cached streaming in grounding requests
- **Search Parameters**: Support for perplexity.ai search options
- **Webhook Enhancement**: Return appropriate status codes for streaming webhook failures

## Guardrails

We've significantly expanded our guardrails capabilities:

- **AWS Bedrock Guardrails**: Native integration with AWS content filtering
- **Mistral Moderations**: Added support for Mistral's moderation endpoint
- **Lasso Integration**: New provider for enhanced content safety
- **Input/Output Format**: New standardized format for setting guardrails
- **Default Headers**: Simplified configuration through new API & SDK headers

## Documentation

<Card icon="book" title="Admin API Introduction" href="https://portkey.ai/docs/api-reference/admin-api/introduction" horizontal />

We've made significant improvements to our documentation:

- **Admin API Docs**: Comprehensive guide to our Control Plane API
- **Enterprise Architecture**: [Updated deployment architecture](https://portkey.ai/docs/product/enterprise-offering/private-cloud-deployments/architecture)
- **Enterprise Code Visibility**: API docs now show code for enterprise deployments
- **Prompt Documentation**: Complete revamp of our prompt engineering guides
- **New Cookbook**: [Building an LLM as a judge](/guides/use-cases/llm-as-judge)

## SDK Updates

- **Custom Headers**: Send headers with `extra_headers` param in any method
- **Private Deployment Tracing**: Instrument LlamaIndex/LangChain with private deployments
- **Support for OpenAI Developer Role**: Full compatibility with OpenAI's new permissions

## Analytics

New filtering capabilities in logs & analytics dashboards:

- Filter requests by cache status:
  - Cache Hit
  - Cache Miss
  - Cache Disabled
  - Cache Semantic Hit

## Community

<Frame>
<img src="/images/changelog/aman-testimonial.jpeg" />
</Frame>

"Describing Portkey as merely useful would be an understatement; it's a must-have." - @AManInTech

### Community Contributors

A special thanks to our community contributors this month:
- [urbanonymous](https://github.com/urbanonymous)
- [vineye25](https://github.com/vineye25)
- [Ignacio](https://github.com/elentaure)
- [Ajay Satish](https://github.com/Ajay-Satish-01)

## Support

<CardGroup cols={2}>
<Card title="Need Help?" icon="bug" href="https://github.com/Portkey-AI/gateway/issues">
Open an issue on GitHub
</Card>
<Card title="Join Us" icon="discord" href="https://portkey.wiki/community">
Get support in our Discord
</Card>
</CardGroup>




<img width="200" src="/images/cake-mask.png" />

**Kicking off 2025 with major releases! ðŸŽ‰**

January marks a milestone for Portkey with our first industry report â€” we analyzed over 2 trillion tokens flowing through Portkey to find out production patterns for LLMs.

We're also expanding our platform capabilities with advanced PII redaction, JWT authentication, comprehensive audit logs, unified files & batches API, and support for private LLMs. Latest LLMs like Deepseek R1, OpenAI o3, and Gemini thinking model are also integrated with Portkey.

Plus, we are attending the [AI Engineer Summit in New York](https://x.com/PortkeyAI/status/1886629690615747020) in February, and hosting in-person meetups in [Mumbai](https://lu.ma/bgiyw0cy) & [NYC](https://lu.ma/vmf0egzl).

Let's dive in!

## Summary

| Area | Key Updates |
| :-- | :-- |
| Benchmark | â€¢ Released [LLMs in Prod Report 2025](https://portkey.ai/llms-in-prod-25) analyzing 2T+ tokens<br/>â€¢ Key finding: Multi-LLM deployment is now standard<br/>â€¢ Average prompt size up 4x, with 40% cost savings from caching |
| Security | â€¢ Advanced PII redaction with automatic standardized identifiers<br/>â€¢ JWT authentication support for enterprise deployments<br/>â€¢ Comprehensive audit logs for all critical actions<br/>â€¢ Enforced metadata schemas for better governance<br/>â€¢  Attach default configs & metadata to API keys<br/>â€¢  Granular workspace management controls |
| Platform | â€¢ Unified API for files & batches across major providers<br/>â€¢ Support for private LLM deployments<br/>â€¢ Enhanced virtual keys with granular controls |
| New Models | â€¢ Deepseek R1 available across 7+ providers<br/>â€¢ Added Gemini thinking model<br/>â€¢ Support for Perplexity Sonar models<br/>â€¢ o3-mini integration |
| Integrations | â€¢ AWS Bedrock Guardrails support<br/>â€¢ Milvus DB & Replicate integrations<br/>â€¢ Expanded Open WebUI support<br/>â€¢ Guardrails for embedding requests |
| Community | â€¢ We did a deep dive into MCP and event-driven architecture for agentic systems |

<Frame>
<img width="700" src="/images/changelog/report-header.png" />
</Frame>

Our comprehensive analysis of 2T+ tokens processed through Portkey's Gateway reveals fascinating insights about how teams are deploying LLMs in production. Here are the key findings:

<CardGroup cols={3}>
<Card title="Multi-LLM is the New Normal">
Despite OpenAI's dominance (>50% of prod traffic), teams are actively implementing multi-LLM strategies for reliability and specialized use cases
</Card>
<Card title="Prompts are Getting Complex">
Average prompt size has increased 4x in the last year, indicating more sophisticated engineering techniques and complex workloads
</Card>
<Card title="Caching is Critical">
Implementation of proper caching strategies leads to up to 40% cost savings - a must-have for production deployments
</Card>
</CardGroup>

<Card icon="lightbulb" title="Read the full LLMs in Prod 2025 Report â†’" href="https://portkey.ai/llms-in-prod-25" horizontal/>

---

## Platform

**Advanced PII Redaction**

We've significantly enhanced Portkey's Guardrails with request mutation capabilities.

When any sensitive data (like email, phone number, SSN) is detected in user requests, our PII redaction automatically replaces it with standardized identifiers before it reaches the LLM. This works seamlessly across our entire guardrails ecosystem, including AWS Bedrock Guardrails, Patronus AI, Promptfoo, Pangea, and more.


**Unified Files & Batches API**

Managing file uploads and batch processing across multiple LLM providers is now dramatically simpler. Instead of building provider-specific integrations, you can:
- **Upload once, use everywhere** - test your data across different foundation models
- **Run A/B tests seamlessly across providers** - Choose between native provider batching or Portkey's custom batch API

**Integrate Private LLMs**

You can now add your privately hosted LLMs to Portkey's virtual keys. Simply:
- Configure your model's base URL
- Set required authentication headers
- Start routing requests through our unified API

This means you can use your private deployments alongside commercial providers, with the same monitoring, reliability, and management features.

**API Keys with Default Configs & Metadata**

You can now attach default Portkey config & Metadata with any API key you create.
- Automatically monitor how a service/user is consuming Portkey API by enforcing metadata
- Apply Guardrails on requests automatically by adding them to Configs and attaching that to the key
- Set default fallbacks for outgoing request

## Enterprise

Running AI at scale requires robust security, visibility, and control. This month, we've launched a comprehensive set of enterprise features to enable that:

#### Authentication & Access Control
- **JWT Authentication**: Secure API access with JWT tokens, with support for JWKS URL and custom claims validation.
- **Workspace Management**: Manage workspace access and control who can view logs or create API keys from the Admin dashboard

#### Governance & Compliance
- **Metadata Schemas**: Enforce standardized request metadata across teams - crucial for governance and cost allocation
- **Audit Logging**: Track every critical action across both the Portkey app and Admin API, with detailed user attribution
- **Security Settings**: Expanded settings for managing logs visibility and API key creation

## Customer Love

After evaluating 17 different platforms, this AI team replaced 2+ years of homegrown tooling with Portkey Prompts.

<Frame>
<img src="/images/changelog/shoutout-1.png" />
</Frame>

They were able to do this because of three things:
- They could build reusable prompts with our partial templates
- Our versioning let them confidently roll out changes
- And they didn't have to refactor anything thanks to our OpenAI-compatible APIs

---


## Integrations

#### Models & Providers

<Card title="Deepseek R1" href="/integrations/llms/deepseek">
Access Deepseek's latest reasoning model through multiple providers: direct API, Fireworks AI, Together AI, Openrouter, Groq, AWS Bedrock, Azure AI Inference, and more.
</Card>

<CardGroup cols={2}>
<Card title="Gemini Thinking Model" href="/integrations/llms/vertex-ai">
To keep things OpenAI compatible, you can decide if you'd like Portkey to return the reasoning tokens or not
</Card>
<Card title="o3-mini" href="/integrations/llms/openai">
Available across both OpenAI & Azure OpenAI
</Card>
<Card title="Perplexity Sonar" href="/integrations/llms/perplexity-ai">
Along with support for their citations and other features
</Card>
<Card title="Replicate" href="/integrations/llms/replicate">
Full support for Replicate's model marketplace
</Card>
</CardGroup>

#### Libraries & Tools
<CardGroup cols={2}>
<Card title="Milvus DB" href="/integrations/vector-databases/milvus">
Direct routing support for Milvus vector database
</Card>
<Card title="Qdrant DB" href="/integrations/vector-databases/qdrany">
Direct routing support for Qdrant vector database
</Card>
<Card title="Open WebUI" href="/integrations/libraries/openwebui">
Expanded integration capabilities
</Card>
<Card title="Langchain" href="/integrations/libraries/langchain-js">
Enhanced documentation and integration guides
</Card>
</CardGroup>

#### Guardrails

**Inverse Guardrail**
All eligible checks now have an `Inverse` option in the UI - which triggers a `TRUE` verdict when the Guardrail verdict fails.

<CardGroup cols={2}>
<Card title="AWS Bedrock Guardrails">
Native support for AWS Bedrock's guardrail capabilities
</Card>
</CardGroup>

**Guardrails on Embedding Requests**
Portkey Guardrails now work on your embedding input requests!

## Community

<Frame>
<img src="/images/changelog/ai-eng-nyc.png" />
</Frame>

We are attending the [AI Engineer Summit in NYC](https://x.com/PortkeyAI/status/1886629690615747020) this February and have some extra event passes to share! Reach out to us [on Discord](https://portkey.wiki/community) to ask for a pass.

We are also hosting small meetups in NYC and Mumbai this month to meet with local engineering leaders and ML/AI platform leads. Register for them below:

<CardGroup cols={2}>
    <Card title="LLMs in Prod Mumbai" href="https://lu.ma/bgiyw0cy" />
<Card title="LLMs in Prod NYC" href="https://lu.ma/vmf0egzl" />
</CardGroup>



## Resources

**EDA for Agents**

<Frame>
<img src="/images/changelog/EDA-for-Agents-Photo.jpg" />
</Frame>

Last month we hosted an inspiring AI practitioners meetup with Ojasvi Yadav and Anudeep Yegireddi to discuss the role of Event-Driven Architecture in building Multi-Agent Systems using and MCP.

[Read event report here â†’](https://portkey.ai/blog/event-driven-architecture-for-ai-agents)

Essential reading for your AI infrastructure:
- [LLMs in Prod Report 2025](https://portkey.ai/llms-in-prod-25): Comprehensive analysis of production LLM usage patterns
- [The Real Cost of Building an LLM Gateway](https://portkey.ai/blog/the-cost-of-building-an-llm-gateway/): Understanding infrastructure investments
- [Critical Role of Audit Logs](https://portkey.ai/blog/beyond-implementation-why-audit-logs-are-critical-for-enterprise-ai-governance/): Enterprise AI governance
- [Error Library](https://portkey.ai/error-library): New documentation covering common errors across 30+ providers
- [Deepseek on Fireworks](https://x.com/PortkeyAI/status/1885231024483033295): How to use Portkey with Fireworks to call Deepseek's R1 model for reasoning tasks

## Improvements

- Token counting is now more accurate for Anthropic streams
- Added logprobs for Vertex AI
- Improved usage object mapping for Perplexity
- Error handling is more robust across all SDKs

---

## Support

<CardGroup cols={2}>
<Card title="Need Help?" icon="bug" href="https://github.com/Portkey-AI/gateway/issues">
Open an issue on GitHub
</Card>
<Card title="Join Us" icon="discord" href="https://portkey.wiki/community">
Get support in our Discord
</Card>
</CardGroup>
