---
title: 'Lemonfox-AI'
description: 'Integrate LemonFox with Portkey for seamless completions, prompt management, and advanced features like streaming, function calling, and fine-tuning.'
---

<Note>
**Portkey Provider Slug:** `lemonfox-ai`
</Note>

## Overview

Portkey offers native integrations with [LemonFox-AI](https://www.lemonfox.ai/) for Node.js, Python, and REST APIs. By combining Portkey with Lemonfox AI, you can create production-grade AI applications with enhanced reliability, observability, and advanced features.

<CardGroup cols={1}>
  <Card title="Lemonfox AI Documentation" icon="book" href="https://www.lemonfox.ai/apis">
    Explore the official Lemonfox AI documentation for comprehensive details on their APIs and models.
  </Card>
</CardGroup>

## Getting Started

<Steps>
  <Step title="Obtain your LemonFox AI API Key">
    Visit the [LemonFox dashboard](https://www.lemonfox.ai/apis/keys) to generate your API key.
  </Step>

  <Step title="Create a Virtual Key in Portkey">
    Portkey's virtual key vault simplifies your interaction with LemonFox AI. Virtual keys act as secure aliases for your actual API keys, offering enhanced security and easier management through [budget limits](/product/ai-gateway/virtual-keys/budget-limits) to control your API usage.

    Use the Portkey app to create a [virtual key](/product/ai-gateway/virtual-keys) associated with your Lemonfox AI API key.
  </Step>

  <Step title="Initialize the Portkey Client">
    Now that you have your virtual key, set up the Portkey client:

    ### Portkey Hosted App
    Use the Portkey API key and the LemonFox AI virtual key to initialize the client in your preferred programming language.

    <CodeGroup>
    ```python Python
    from portkey_ai import Portkey

    portkey = Portkey(
        api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
        virtual_key="VIRTUAL_KEY"   # Replace with your virtual key for LemonFox AI
    )
    ```

    ```javascript Node.js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
        apiKey: "PORTKEY_API_KEY", // defaults to process.env["PORTKEY_API_KEY"]
        virtualKey: "VIRTUAL_KEY" // Your LemonFox AI Virtual Key
    })
    ```
    </CodeGroup>

    ### Open Source Use
    Alternatively, use Portkey's Open Source AI Gateway to enhance your app's reliability with minimal code:

    <CodeGroup>
    ```python Python
    from portkey_ai import Portkey, PORTKEY_GATEWAY_URL

    portkey = Portkey(
        api_key="dummy",  # Replace with your Portkey API key
        base_url=PORTKEY_GATEWAY_URL,
        Authorization="LEMONFOX_AI_API_KEY", # Replace with your Lemonfox AI API Key
        provider="lemonfox-ai"
    )
    ```

    ```javascript Node.js
    import Portkey, { PORTKEY_GATEWAY_URL } from 'portkey-ai'

    const portkey = new Portkey({
        apiKey: "dummy", // Replace with your Portkey API key
        baseUrl: PORTKEY_GATEWAY_URL,
        Authorization: "LEMONFOX_AI_API_KEY", // Replace with your Lemonfox AI API Key
        provider: "lemonfox-ai"
    })
    ```
    </CodeGroup>
  </Step>
</Steps>

ðŸ”¥ That's it! You've integrated Portkey into your application with just a few lines of code. Now let's explore making requests using the Portkey client.

## Supported Models

<Accordion title="Supported Lemonfox AI Models">

`Chat` - Mixtral AI, Llama 3.1 8B and Llama 3.1 70B


`Speech-To-Text`- Whisper large-v3


`Vision`- Stable Diffusion XL (SDXL)


</Accordion>


## Supported Endpoints and Parameters

| Endpoint          | Supported Parameters                                                                                                                                                     |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `chatComplete`    | messages, max_tokens, temperature, top_p, stream, presence_penalty, frequency_penalty   |
| `imageGenerate`   | prompt, response_format, negative_prompt, size, n                                                                                                            |
| `createTranscription` | translate, language, prompt, response_format, file                                                                                                                                       |                                                                                                                                      |


## Lemonfox AI Supported Features

### Chat Completions

Generate chat completions using Lemonfox AI models through Portkey:

<CodeGroup>
```python Python
completion = portkey.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}],
    model="llama-8b-chat"
)

print(completion.choices[0].message.content)
```

```javascript Node.js
const chatCompletion = await portkey.chat.completions.create({
    messages: [{ role: 'user', content: 'Say this is a test' }],
    model: 'llama-8b-chat',
});

console.log(chatCompletion.choices[0].message.content);
```

```curl REST
curl -X POST "https://api.portkey.ai/v1/chat/completions" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -d '{
       "messages": [{"role": "user", "content": "Say this is a test"}],
       "model": "llama-8b-chat"
     }'
```
</CodeGroup>

### Streaming

Stream responses for real-time output in your applications:

<CodeGroup>
```python Python
chat_complete = portkey.chat.completions.create(
    model="llama-8b-chat",
    messages=[{"role": "user", "content": "Say this is a test"}],
    stream=True
)

for chunk in chat_complete:
    print(chunk.choices[0].delta.content or "", end="", flush=True)
```

```javascript Node.js
const stream = await portkey.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Say this is a test' }],
  stream: true,
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || '');
}
```

```curl REST
curl -X POST "https://api.portkey.ai/v1/chat/completions" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -d '{
       "model": "llama-8b-chat",
       "messages": [{"role": "user", "content": "Say this is a test"}],
       "stream": true
     }'
```
</CodeGroup>

### Image Generate

Here's how you can generate images using Lemonfox AI

<CodeGroup>



```python Python
from portkey_ai import Portkey

client = Portkey(
  api_key = "PORTKEY_API_KEY",
  virtual_key = "PROVIDER_VIRTUAL_KEY"
)

client.images.generate(
  prompt="A cute baby sea otter",
  n=1,
  size="1024x1024"
)

```

```javascript Node.js
import Portkey from 'portkey-ai';

const client = new Portkey({
  apiKey: 'PORTKEY_API_KEY',
  virtualKey: 'PROVIDER_VIRTUAL_KEY'
});

async function main() {
  const image = await client.images.generate({prompt: "A cute baby sea otter" });

  console.log(image.data);
}
main();

```

```curl REST
curl https://api.portkey.ai/v1/images/generations \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-virtual-key: $PORTKEY_PROVIDER_VIRTUAL_KEY" \
  -d '{
    "prompt": "A cute baby sea otter",
    "n": 1,
    "size": "1024x1024"
  }'

```
</CodeGroup>

### Transcription

Portkey supports both `Transcription`  methods for STT models:

<CodeGroup>
```python Python
audio_file= open("/path/to/file.mp3", "rb")

# Transcription
transcription = portkey.audio.transcriptions.create(
  model="whisper-1",
  file=audio_file
)
print(transcription.text)
```

```javascript Node.js
import fs from "fs";

// Transcription
async function transcribe() {
  const transcription = await portkey.audio.transcriptions.create({
    file: fs.createReadStream("/path/to/file.mp3"),
    model: "whisper-1",
  });
  console.log(transcription.text);
}
transcribe();


```

```curl REST
# Transcription
curl -X POST "https://api.portkey.ai/v1/audio/transcriptions" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@/path/to/file.mp3" \
     -F "model=whisper-1"

```
</CodeGroup>




# Portkey's Advanced Features

## Track End-User IDs

Portkey allows you to track user IDs passed with the user parameter in Lemonfox AI requests, enabling you to monitor user-level costs, requests, and more:

<CodeGroup>
```python Python
response = portkey.chat.completions.create(
  model="llama-8b-chat",
  messages=[{"role": "user", "content": "Say this is a test"}],
  user="user_123456"
)
```

```javascript Node.js
const chatCompletion = await portkey.chat.completions.create({
  messages: [{ role: "user", content: "Say this is a test" }],
  model: "llama-8b-chat",
  user: "user_12345",
});
```

```curl REST
curl -X POST "https://api.portkey.ai/v1/chat/completions" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -d '{
       "model": "llama-8b-chat",
       "messages": [{"role": "user", "content": "Say this is a test"}],
       "user": "user_123456"
     }'
```
</CodeGroup>

When you include the user parameter in your requests, Portkey logs will display the associated user ID, as shown in the image below:

<img src="/images/llms/logs.png" alt="Portkey Logs with User ID" />

In addition to the `user` parameter, Portkey allows you to send arbitrary custom metadata with your requests. This powerful feature enables you to associate additional context or information with each request, which can be useful for analysis, debugging, or other custom use cases.

<CardGroup cols={1}>
  <Card title="Learn More About Metadata" icon="tags" href="/product/observability/metadata">
    Explore how to use custom metadata to enhance your request tracking and analysis.
  </Card>
</CardGroup>

## Using The Gateway Config

Here's a simplified version of how to use Portkey's Gateway Configuration:

<Steps>
  <Step title="Create a Gateway Configuration" titleSize="h3">
    You can create a Gateway configuration using the Portkey Config Dashboard or by writing a JSON configuration in your code. In this example, requests are routed based on the user's subscription plan (paid or free).

    ```json
    config = {
      "strategy": {
        "mode": "conditional",
        "conditions": [
          {
            "query": { "metadata.user_plan": { "$eq": "paid" } },
            "then": "llama-8b-chat"
          },
          {
            "query": { "metadata.user_plan": { "$eq": "free" } },
            "then": "gpt-3.5"
          }
        ],
        "default": "base-gpt4"
      },
      "targets": [
        {
          "name": "llama-8b-chat",
          "virtual_key": "xx"
        },
        {
          "name": "gpt-3.5",
          "virtual_key": "yy"
        }
      ]
    }
    ```
  </Step>

  <Step title="Process Requests" titleSize="h3">
    When a user makes a request, it will pass through Portkey's AI Gateway. Based on the configuration, the Gateway routes the request according to the user's metadata.
    <img src="/images/llms/conditional-routing.png" alt="Conditional Routing Diagram" />
  </Step>

  <Step title="Set Up the Portkey Client" titleSize="h3">
    Pass the Gateway configuration to your Portkey client. You can either use the config object or the Config ID from Portkey's hosted version.

    <CodeGroup>
    ```python Python
    from portkey_ai import Portkey

    portkey = Portkey(
        api_key="PORTKEY_API_KEY",
        virtual_key="VIRTUAL_KEY",
        config=portkey_config
    )
    ```

    ```javascript Node.js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
      apiKey: "PORTKEY_API_KEY",
      virtualKey: "VIRTUAL_KEY",
      config: portkeyConfig
    })
    ```
    </CodeGroup>
  </Step>
</Steps>

That's it! Portkey seamlessly allows you to make your AI app more robust using built-in gateway features. Learn more about advanced gateway features:

<CardGroup cols={2}>
  <Card title="Load Balancing" icon="link" href="/product/ai-gateway/load-balancing">
    Distribute requests across multiple targets based on defined weights.
  </Card>
  <Card title="Fallbacks" icon="life-ring" href="/product/ai-gateway/fallbacks">
    Automatically switch to backup targets if the primary target fails.
  </Card>
  <Card title="Conditional Routing" icon="route" href="/product/ai-gateway/conditional-routing">
    Route requests to different targets based on specified conditions.
  </Card>
  <Card title="Caching" icon="database" href="/product/ai-gateway/cache-simple-and-semantic">
    Enable caching of responses to improve performance and reduce costs.
  </Card>
</CardGroup>

## Guardrails

Portkey's AI gateway enables you to enforce input/output checks on requests by applying custom hooks before and after processing. Protect your user's/company's data by using PII guardrails and many more available on Portkey Guardrails:

```json
{
	"virtual_key":"lemonfox-ai-xxx",
	"before_request_hooks": [{
		"id": "input-guardrail-id-xx"
	}],
	"after_request_hooks": [{
		"id": "output-guardrail-id-xx"
	}]
}
```

<Card title="Learn More About Guardrails" icon="shield-check" href="/product/guardrails">
  Explore Portkey's guardrail features to enhance the security and reliability of your AI applications.
</Card>

## Next Steps

The complete list of features supported in the SDK are available in our comprehensive documentation:

<Card title="Portkey SDK Documentation" icon="book-open" href="/api-reference/portkey-sdk-client">
  Explore the full capabilities of the Portkey SDK and how to leverage them in your projects.
</Card>

---



For the most up-to-date information on supported features and endpoints, please refer to our [API Reference](/docs/api-reference/introduction).
