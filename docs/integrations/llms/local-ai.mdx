---
title: "LocalAI"
---

Portkey provides a robust and secure gateway to facilitate the integration of various Large Language Models (LLMs) into your applications, including your **locally hosted models through** [**LocalAI**](https://localai.io/).

## Portkey SDK Integration with LocalAI

### 1\. Install the Portkey SDK
<Tabs>
    <Tab title="NodeJS">
        ```
        npm install --save portkey-ai
        ```
    </Tab>
    <Tab title="Python">
      
```
pip install portkey-ai
```
    </Tab>

  </Tabs>



### 2\. Initialize Portkey with LocalAI URL

First, ensure that your API is externally accessible. If you're running the API on `http://localhost`, consider using a tool like `ngrok` to create a public URL. Then, instantiate the Portkey client by adding your LocalAI URL (along with the version identifier) to the `customHost` property, and add the provider name as `openai`.
<Note>
**Note:** Don't forget to include the version identifier (e.g., `/v1`) in the `customHost` URL
</Note>
<Tabs>
    <Tab title="NodeJS SDK">
        ```js
        import Portkey from 'portkey-ai'
         
        
        const portkey = new Portkey({
        
            apiKey: "PORTKEY_API_KEY", // defaults to process.env["PORTKEY_API_KEY"]
        
            provider: "openai",
        
            customHost: "https://7cc4-3-235-157-146.ngrok-free.app/v1" // Your LocalAI ngrok URL
        
        })
        ```
    </Tab>
    <Tab title="Python SDK">
        ```python
        from portkey_ai import Portkey
        
        portkey = Portkey(
        
            api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
        
            provider="openai",
        
            custom_host="https://7cc4-3-235-157-146.ngrok-free.app/v1" # Your LocalAI ngrok URL    
        
        )
        ```
    </Tab>

  </Tabs>


<Note>
Portkey currently supports all endpoints that adhere to the OpenAI specification. This means, you can access and observe any of your LocalAI models that are exposed through OpenAI-compliant routes. 
</Note>
[List of supported endpoints here](/docs/integrations/llms/local-ai#localai-endpoints-supported).

### 3\. Invoke Chat Completions

Use the Portkey SDK to invoke chat completions from your LocalAI model, just as you would with any other provider.
<Tabs>
    <Tab title="NodeJS SDK">
        ```js
        const chatCompletion = await portkey.chat.completions.create({
        
            messages: [{ role: 'user', content: 'Say this is a test' }],
        
            model: 'ggml-koala-7b-model-q4_0-r2.bin',
        
        });
        
        console.log(chatCompletion.choices);
        ```
    </Tab>
    <Tab title="Python SDK">
        ```python
        completion = portkey.chat.completions.create(
        
            messages= [{ "role": 'user', "content": 'Say this is a test' }],
        
            model= 'ggml-koala-7b-model-q4_0-r2.bin'
        
        )
        
        print(completion)
        ```
    </Tab>

  </Tabs>



## LocalAI Endpoints Supported

| /chat/completions (Chat, Vision, Tools support) | [Doc](/docs/provider-endpoints/chat)                             |
| ----------------------------------------------- | ---------------------------------------------------------------- |
| /images/generations                             | [Doc](/docs/provider-endpoints/images/create-image)              |
| /embeddings                                     | [Doc](/docs/provider-endpoints/embeddings)                       |
| /audio/transcriptions                           | [Doc](/docs/product/ai-gateway/multimodal-capabilities/speech-to-text) |

## Next Steps

Explore the complete list of features supported in the SDK:
<Card title="SDK" href="/docs/api-reference/portkey-sdk-client">
</Card>

---

You'll find more information in the relevant sections:

1. [Add metadata to your requests](/docs/product/observability/metadata)
2. [Add gateway configs to your Ollama requests](/docs/product/ai-gateway/universal-api#ollama-in-configs)
3. [Tracing Ollama requests](/docs/product/observability/traces)
4. [Setup a fallback from OpenAI to Ollama APIs](/docs/product/ai-gateway/fallbacks)