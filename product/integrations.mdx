---
title: Integrations
description: A step-by-step guide for organization admins to set up their first integration.
---

# **Org Admin Quickstart: Setting Up Your First Model Catalog Integration**

As an Organization Admin, the Model Catalog gives you the power to centrally manage, provision, and govern all AI models used across your company. This guide will walk you through creating your first centralized integration.

**Prerequisite:** The Model Catalog feature has been enabled for your organization by the Portkey team.

### **Navigating to the Integrations Hub**

1.  Click on your organization name in the bottom-left corner of the sidebar.
2.  In the menu that appears, select **Integrations**. This is your new central dashboard for all provider connections.

    

### **Step 1: Connect a New Provider Integration**

This step is similar to creating a Virtual Key, but it's happening at the organization level.

1.  From the **Integrations** page, find the provider you want to connect (e.g., OpenAI, Azure OpenAI, AWS Bedrock) and click **Connect**.
2.  Fill in the details:
    *   **Integration Name:** A friendly name for you to identify this connection (e.g., "Azure Production - US East").
    *   **Slug:** A unique, URL-friendly identifier. This will be used by developers to call models (e.g., `azure-prod-useast`).
    *   **Credentials:** Securely enter your API keys or other authentication details. These are encrypted and will not be visible after saving.
3.  Click **Next**.

### **Step 2: Provision the Integration to Workspaces**

Here, you decide which teams get access to this provider and under what conditions.

1.  You will see a list of all workspaces within your organization.
2.  Use the toggle next to a workspace name to **enable or disable** access.
3.  For each enabled workspace, you can optionally click **Edit Budget & Rate Limits** to set specific spending caps or request limits that apply *only to that workspace* for this integration.
4.  **(Optional) For New Workspaces:** Toggle on **"Automatically provision this integration for new workspaces"** to ensure any future teams automatically get access with a default budget/rate limit you define.
5.  Click **Next**.

    

### **Step 3: Provision Specific Models**

This is where you enforce model governance.

1.  You will see a list of all models available from the provider you're connecting.
2.  By default, all models may be selected. You can **Clear all** and then select only the models you wish to approve for use.
3.  **(Optional) For Dynamic Models:** If you're using a provider like Fireworks AI with many community models, you can toggle on **"Automatically enable new models"**. This is useful, but for providers like OpenAI or Azure, we recommend an explicit allow-list for better cost control.
4.  Click **Create Integration**.

**That's it!** You have successfully created a centrally managed integration. The workspaces you provisioned will now see this as an available "AI Provider" in their Model Catalog, with access only to the models you specified and constrained by the budgets you set. You can now repeat this process for all your providers.

