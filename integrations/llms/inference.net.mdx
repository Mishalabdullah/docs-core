---
title: "Inference.net"
description: "Portkey provides a robust and secure gateway to facilitate the integration of various Large Language Models (LLMs) into your applications, including the models hosted on [Inference.net](https://www.inference.net/).
"
---


<Note>
Provider slug: `inference-net`
</Note>

## Portkey SDK Integration with Inference.net

Portkey provides a consistent API to interact with models from various providers. To integrate Inference.net with Portkey:

### 1. Install the Portkey SDK

<Tabs>
  <Tab title="NodeJS">
    ```sh
    npm install --save portkey-ai
    ```
  </Tab>
  <Tab title="Python">
    ```sh
    pip install portkey-ai
    ```
  </Tab>
</Tabs>

### 2. Initialize Portkey with Inference.net Authorization

* Set `provider` name as `inference-net`
* Pass your API key with `Authorization` header

<Tabs>
  <Tab title="NodeJS SDK">
    ```javascript
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
        apiKey: "PORTKEY_API_KEY", // defaults to process.env["PORTKEY_API_KEY"]
        provider: "inference-net",
        Authorization: "Bearer INFERENCE-NET API KEY"
    })
    ```
  </Tab>
  <Tab title="Python SDK">
    ```python
    from portkey_ai import Portkey
    portkey = Portkey(
        api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
        provider="inference-net",
        Authorization="Bearer INFERENCE-NET API KEY"
    )
    ```
  </Tab>
</Tabs>

### 3. Invoke Chat Completions

<Tabs>
  <Tab title="NodeJS SDK">
    ```javascript
    const chatCompletion = await portkey.chat.completions.create({
        messages: [{ role: 'user', content: 'Say this is a test' }],
        model: 'llama3',
    });
    console.log(chatCompletion.choices);
    ```
  </Tab>
  <Tab title="Python SDK">
    ```python
    completion = portkey.chat.completions.create(
        messages= [{ "role": 'user', "content": 'Say this is a test' }],
        model= 'llama3'
    )
    print(completion)
    ```
  </Tab>
</Tabs>

## Supported Models

Find more info about models supported by Inference.net here:

[Inference.net](https://www.inference.net/)

## Next Steps

The complete list of features supported in the SDK are available on the link below.

<Card title="SDK" href="/api-reference/portkey-sdk-client">
</Card>

You'll find more information in the relevant sections:

1. [Add metadata to your requests](/product/observability/metadata)
2. [Add gateway configs to your Inference.net requests](/product/ai-gateway/configs)
3. [Tracing Inference.net requests](/product/observability/traces)
4. [Setup a fallback from OpenAI to Inference.net](/product/ai-gateway/fallbacks)
