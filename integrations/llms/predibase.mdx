---
title: "Predibase"
---

Portkey provides a robust and secure gateway to seamlessly integrate **open-source** and **fine-tuned** LLMs from Predibase into your applications. With Portkey, you can leverage powerful features like fast AI gateway, caching, observability, prompt management, and more, while securely managing your LLM API keys through a virtual key system.
<Note>
Provider Slug. **`predibase`**
</Note>
## Portkey SDK Integration with Predibase

Using Portkey, you can call your Predibase models in the familar **OpenAI-spec** and try out your existing pipelines on Predibase fine-tuned models with 2 LOC change.

### 1\. Install the Portkey SDK

Install the Portkey SDK in your project using npm or pip:
<Tabs>
  <Tab title="NodeJS">
    ```sh
    npm install --save portkey-ai
    ```
  </Tab>
  <Tab title="Python">
```sh
pip install portkey-ai
```
  </Tab>

</Tabs>




### 2\. Initialize Portkey with the Virtual Key

To use Predibase with Portkey, [get your API key from here](https://app.predibase.com/settings), then add it to Portkey to create the virtual key.
<Tabs>
  <Tab title="NodeJS SDK">
    ```js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
        apiKey: "PORTKEY_API_KEY", // defaults to process.env["PORTKEY_API_KEY"]
        virtualKey: "VIRTUAL_KEY" // Your Predibase Virtual Key
    })
    ```
  </Tab>
  <Tab title="Python SDK">
```python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
    virtual_key="VIRTUAL_KEY"   # Replace with your virtual key for Predibase
)
```
  </Tab>

  <Tab title="OpenAI Node SDK">
```js
import OpenAI from "openai";
import { PORTKEY_GATEWAY_URL, createHeaders } from "portkey-ai";

const portkey = new OpenAI({
  baseURL: PORTKEY_GATEWAY_URL,
  defaultHeaders: createHeaders({
    apiKey: "PORTKEY_API_KEY",
    virtualKey: "PREDIBASE_VIRTUAL_KEY",
  }),
});
```
  </Tab>
  <Tab title="OpenAI Python SDK">
```python
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

portkey = OpenAI(
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        api_key="PORTKEY_API_KEY",
        virtual_key="PREDIBASE_VIRTUAL_KEY"
    )
)
```
  </Tab>
</Tabs>




### 3\. Invoke Chat Completions on Predibase Serverless Endpoints

Predibase offers LLMs like **Llama 3**, **Mistral**, **Gemma**, etc. on its [serverless infra](https://docs.predibase.com/user-guide/inference/models#serverless-endpoints) that you can query instantly.
<Note>
#### Sending Predibase Tenand ID

Predibase expects your **account tenant ID** along with the API key in each request. With Portkey, you can send [**your Tenand ID**](https://app.predibase.com/settings) with the `**user**` param while making your request.
</Note>
<Tabs>
  <Tab title="Portkey OR OpenAI NodeJS SDK">

    ```js
    const chatCompletion = await portkey.chat.completions.create({
        messages: [{ role: 'user', content: 'Say this is a test' }],
        model: 'llama-3-8b	',
        user: 'PREDIBASE_TENANT_ID'
    });

    console.log(chatCompletion.choices);
    ```
  </Tab>
  <Tab title="Portkey OR OpenAI Python SDK">
```python
completion = portkey.chat.completions.create(
    messages= [{ "role": 'user', "content": 'Say this is a test' }],
    model= 'llama-3-8b',
    user= "PREDIBASE_TENANT_ID"
)

print(completion)
```
  </Tab>
<Tab title="REST API">
  ```sh
  curl https://api.portkey.ai/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "x-portkey-api-key: $PORTKEY_API_KEY" \
    -H "x-portkey-virtual-key: $PREDIBASE_VIRTUAL_KEY" \
    -d '{
      "messages": [{"role": "user","content": "Hello!"}],
      "model": "llama-3-8b",
      "user": "PREDIBASE_TENANT_ID"
    }'
  ```
  </Tab>

</Tabs>



### 4\. Invoke Predibase Fine-Tuned Models

With Portkey, you can send your fine-tune model & adapter details directly with the `model` param while making a request.
<Note>
The format is:

`model = <base_model>:<adapter-repo-name/adapter-version-number>`
</Note>

For example, if your base model is `llama-3-8b` and the adapter repo name is `sentiment-analysis`, you can make a request like this:

<Tabs>
  <Tab title="Portkey OR OpenAI NodeJS SDK">
    ```js
    const chatCompletion = await portkey.chat.completions.create({
        messages: [{ role: 'user', content: 'Say this is a test' }],
        model: 'llama-3-8b:sentiment-analysis/1',
        user: 'PREDIBASE_TENANT_ID'
    });

    console.log(chatCompletion.choices);
    ```
  </Tab>
  <Tab title="Portkey OR OpenAI Python SDK">
```python
completion = portkey.chat.completions.create(
    messages= [{ "role": 'user', "content": 'Say this is a test' }],
    model= 'llama-3-8b:sentiment-analysis/1',
    user= "PREDIBASE_TENANT_ID"
)

print(completion)
```
  </Tab>
<Tab title="REST API">
  ```sh
  curl https://api.portkey.ai/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "x-portkey-api-key: $PORTKEY_API_KEY" \
    -H "x-portkey-virtual-key: $PREDIBASE_VIRTUAL_KEY" \
    -d '{
      "messages": [{"role": "user","content": "Hello!"}],
      "model": "llama-3-8b:sentiment-analysis/1",
      "user": "PREDIBASE_TENANT_ID"
    }'
  ```
  </Tab>

</Tabs>



---

### Routing to Dedicated Deployments

Using Portkey, you can easily route to your dedicatedly deployed models as well. Just pass the dedicated deployment name in the `model` param:

<Note>
`model = "my-dedicated-mistral-deployment-name"`
</Note>
### JSON Schema Mode

You can enforce JSON schema for all Predibase models - just set the `response_format` to `json_object` and pass the relevant schema while making your request. Portkey logs will show your JSON output separately

<Tabs>
  <Tab title="Portkey OR OpenAI NodeJS SDK">
    ```js
    const chatCompletion = await portkey.chat.completions.create({
        messages: [{ role: 'user', content: 'Say this is a test' }],
        model: 'llama-3-8b	',
        user: 'PREDIBASE_TENANT_ID',
        response_format: {
          "type": "json_object",
          "schema": {"properties": {
            "name": {"maxLength": 10, "title": "Name", "type": "string"},
            "age": {"title": "Age", "type": "integer"},
            "required": ["name", "age", "strength"],
            "title": "Character",
            "type": "object"
          }
        }
    });

    console.log(chatCompletion.choices);
    ```
  </Tab>
  <Tab title="Portkey OR OpenAI Python SDK">
```python
# Using Pydantic to define the schema
from pydantic import BaseModel, constr

# Define JSON Schema
class Character(BaseModel):
    name: constr(max_length=10)
    age: int
    strength: int

completion = portkey.chat.completions.create(
    messages= [{ "role": 'user', "content": 'Say this is a test' }],
    model= 'llama-3-8b',
    user= "PREDIBASE_TENANT_ID",
    response_format={
        "type": "json_object",
        "schema": Character.schema(),
    },
)

print(completion)
```
  </Tab>
<Tab title="REST API">
  ```sh
  curl https://api.portkey.ai/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "x-portkey-api-key: $PORTKEY_API_KEY" \
    -H "x-portkey-virtual-key: $PREDIBASE_VIRTUAL_KEY" \
    -d '{
      "messages": [{"role": "user","content": "Hello!"}],
      "model": "llama-3-8b",
      "user": "PREDIBASE_TENANT_ID",
      "response_format": {
        "type": "json_object",
        "schema": {"properties": {
          "name": {"maxLength": 10, "title": "Name", "type": "string"},
          "age": {"title": "Age", "type": "integer"},
          "required": ["name", "age", "strength"],
          "title": "Character",
          "type": "object"
        }
      }
    }'
  ```
  </Tab>

</Tabs>



---

## Next Steps

The complete list of features supported in the SDK are available on the link below.

<Card title="SDK" href="/api-reference/portkey-sdk-client">
</Card>

You'll find more information in the relevant sections:

1. [Add metadata to your requests](/docs/product/observability/metadata)
2. [Add gateway configs to your Predibase requests](/docs/product/ai-gateway/configs)
3. [Tracing Predibase requests](/docs/product/observability/traces)
4. [Setup a fallback from OpenAI to Predibase](/docs/product/ai-gateway/fallbacks)
