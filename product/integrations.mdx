---
title: LLM Integrations
description: A step-by-step guide for organization admins to set up their first integration.
---

The **Integrations** page is the central command center for Organization Admins. It's where you securely manage all third-party LLM provider credentials and govern their use across all workspaces from a single, unified dashboard.

This "create once, provision many" model saves significant time, reduces configuration errors, and gives you complete oversight of your AI stack.

### **Understanding the Integrations Dashboard**

The Integrations page is organized into three tabs, each serving a distinct purpose:

*   **`All`**: This is a comprehensive list of all 50+ providers Portkey supports. This is your starting point for connecting a new provider to your organization.
*   **`Connected`**: This tab lists all the integrations that you have personally connected at the organization level. It's your primary view for managing your centrally-governed providers.
*   **`Workspace-Created`**: This tab gives you complete visibility and governance over any integrations created *by Workspace Admins* for their specific workspaces. It ensures that even with delegated control, you maintain a full audit trail and can manage these instances if needed.

---

### **Creating and Provisioning a New Integration**

This guide walks you through connecting a new provider and making it available to your workspaces.

#### **Step 1: Connect the Provider**

<Info>
If you are an existing Portkey user, this step is similar to creating a Virtual Key, but it's happening at the organization level.
</Info>

1.  From the **`All`** tab, find the provider you want to connect (e.g., OpenAI, Azure OpenAI, AWS Bedrock) and click **Connect**.
2.  Fill in the details:
    *   **Integration Name:** A friendly name for you to identify this connection (e.g., "Azure Production - US East").
    *   **Slug:** A unique, URL-friendly identifier. This will be used by developers to call models (e.g., `azure-prod-useast`).
    *   **Credentials:** Securely enter your API keys or other authentication details. These are encrypted and will not be visible after saving.
3.  Click **Next**.

#### **Step 2: Provision to Workspaces**

Here, you decide which teams get access to this provider and under what conditions.

1.  You will see a list of all workspaces within your organization.
2.  Use the toggle next to a workspace name to **enable or disable** access.
3.  For each enabled workspace, you can optionally click **Edit Budget & Rate Limits** to set specific spending caps or request limits that apply *only to that workspace* for this integration.
4.  **(Optional) For Provisioning to New Workspaces:** Toggle on **"Automatically provision this integration for new workspaces"** to ensure any future teams automatically get access with a default budget/rate limit you define.
5.  Click **Next**.

#### **Step 3: Provision Specific Models**

This is where you enforce model governance and control costs.

1.  You will see a list of all models available from the provider you're connecting.
2.  You can **Clear all** and then select only the models you wish to approve for use.
3.  **(Optional) For Dynamic Models:** If you're using a provider like Fireworks AI with many community models, you can toggle on **"Automatically enable new models"**. For providers like OpenAI or Azure, we recommend an explicit allow-list for better cost control.
4.  Click **Create Integration**.

**That's it!** You have successfully created and provisioned a centrally managed integration. It will now appear in your **`Connected`** tab. The workspaces you provisioned will see this as an available "AI Provider" in their Model Catalog, with access only to the models you specified and constrained by the budgets you set.