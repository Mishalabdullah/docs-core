---
title: "October '24"
---

# üéÉ ü™î Portkey in October

October was packed with treats (no tricks!) for Portkey. As we celebrate Halloween and Diwali, we're lighting up your AI infrastructure with some exciting updates. Let's dive in!


<Update label="Highlights">

- **Guardrails are now GA**: Our guardrails feature is now generally available, helping you enforce LLM behavior in realtime. [(*Docs*)](/product/guardrails)
- **Enterprise Updates**: Refreshed our [enterprise offering](https://portkey.ai/docs/product/enterprise-offering) and [welcomed](https://x.com/PortkeyAI/status/1841292805454643393) one of the world's largest tech companies to the Portkey family.
- **Featured in Media**: Check out our why we're building *DevOps for AI* in the [People+AI Newsletter](https://sreeramsridhar.substack.com/p/building-the-devops-for-ai) and our CEO's interview on [Pulse2](https://pulse2.com/portkey-profile-rohit-agarwal-interview/).
- **Anthropic Prompt Caching in Playground**: For Anthropic models, you can now enable any message to be cached right on the prompt playground.
- **Portkey Tops Agent Ops Tooling Benchmark**: Portkey provides 11 critical capabilities to put agents in production. [*Link*](https://x.com/PortkeyAI/status/1851596076488479001)

</Update>

<Update label="Features">

- **AWS Auth**: AWS Assume Role Support for Bedrock. [(*Docs*)](/product/ai-gateway/virtual-keys/bedrock-amazon-assumed-role)
- **Resend User Invite API**: Launched a new API to re-invite a user to your org. [(*Docs*)](/api-reference/admin-api/control-plane/admin/user-invites/resend-a-user-invite)
- **New API Specs**: We also added API specs for: [Prompt Completions API](/api-reference/inference-api/prompts/prompt-completion), [Prompt Render API](/api-reference/inference-api/prompts/prompt-render), and [Inserting Logs API](/api-reference/admin-api/data-plane/logs/insert-a-log)
- **.NET (C#) Support**: See how you can integrate Portkey in your .NET app easily using the OpenAI library and get advanced monitoring, routing, and enterprise features. [(*Docs*)](/api-reference/inference-api/sdks/c-sharp)
- **New OpenAI Param**: OpenAI's `max_completion_tokens` is now supported <Tooltip tip="This means you can send either the `max_completion_tokens` or `max_tokens` param in your requests and Portkey will handle both correctly">across all providers.</Tooltip>
- **Calculating Cached Requests**:Updated cost calculations for cached responses for both OpenAI & Azure OpenAI.
- **JSON Mode**: We now support JSON mode for Gemini models.
- **Controlled Generations**: Gemini Controlled Generations is now supported on Portkey (alnog with support for Pydantic!)
- **Bedrock Converse API**: We've integrated Bedrock's Converse API for all `/chat/completions` requests

</Update>

<Update label="Providers">
We added 6 new providers on the Gateway this month!
<CardGroup cols={3}>
<Card title="Lemonfox" href="/integrations/llms/lemon-fox"></Card>
<Card title="Lambda Labs" href="/integrations/llms/lambda"></Card>
<Card title="Dashscope" href="/integrations/llms/dashscope"></Card>
<Card title="Upstage" href="/integrations/llms/upstage"></Card>
<Card title="Github"></Card>
<Card title="vLLM" href="/integrations/llms/vllm"></Card>
</CardGroup>

</Update>

<Update label="Models">
In true multimodal fashion, we added the all new SDv3 model across a bunch of providers, as well as Google's Imagen model, and more!
<CardGroup cols={2}>
<Card title="Stable Diffusion v3" href="/api-reference/inference-api/images/create-image">Aacross [Stability AI](/integrations/llms/stability-ai), [Fireworks](/integrations/llms/fireworks), [AWS Bedrock](/integrations/llms/aws-bedrock), and [Segmind](/integrations/llms/segmind)</Card>
<Card title="Llama 3.2">Across [Fireworks](/integrations/llms/fireworks), [AWS Bedrock](/integrations/llms/aws-bedrock), [Groq](/integrations/llms/groq), [Together AI](/integrations/llms/together-ai)</Card>
<Card title="Vertex English & Multilingual Embeddings" href="/integrations/llms/vertex-ai#text-embedding-models"></Card>
<Card title="Imagen on Google Vertex" href="/api-reference/inference-api/images/create-image"></Card>
</CardGroup>

</Update>

<Update label="Guardrails">
As we continued to develop more Guardrails
<CardGroup cols={2}>
<Card title="Lowercase Detection">Check if the given string is lowercase or not.</Card>
<Card title="Custom Webhooks">Along with the Webhook information, you can now send any custom metadata along with your request</Card>
<Card title="LLM-based Guardrails">Portkey's LLM-based Guardrails are now updated. You can do:<br />- PII Detection<br />- Language Detection<br />- Moderation<br />- Gibberish Detection</Card>
</CardGroup>
</Update>

<Update label="Integrations">
And also pushed out some really important integrations that help everyone from IT Admins to SDEs, to Indie hackers!
<CardGroup cols={2}>
<Card title="LibreChat for Portkey" href="https://github.com/timmanik/librechat-for-portkey">[Tim](https://www.linkedin.com/in/tim-manik/) wrote up a way to send unique user IDs from LibreChat back to Portkey. Very useful if you‚Äôre a system admin, and you‚Äôre looking to track the costs/user on a centralized instance of LibreChat.</Card>
<Card title="MindsDB" href="/integrations/libraries/mindsdb">Connect your databases, vector stores, and apps to 250+ LLMs with enterprise-grade monitoring and reliability built-in.</Card>
<Card title="OpenWebUI" href="/integrations/libraries/openwebui">Portkey is the only plugin you‚Äôll need for your model management, cost tracking, observability, metadata logging, and more for your Open WebUI instance.</Card>
<Card title="ToolJet" href="/integrations/libraries/tooljet">Add AI-powered capabilities such as chat completions and automations into your ToolJet apps easily</Card>
</CardGroup>

</Update>

<Update label="Resources">

#### 2-Min Guides
- Guide to Prompt Caching [link](https://x.com/PortkeyAI/status/1843209780627997089)
- Building Prod-Ready Apps with Vercel [link](https://x.com/PortkeyAI/status/1844675148609204615)
- OpenAI Swarm Cheat Sheet [link](https://x.com/jumbld/status/1846909380354064526)

#### Longer Guides
- How to Build Multi-Agent AI Systems with OpenAI Swarm & Secure Them Using Portkey [link](https://www.youtube.com/watch?v=9qLkAEJol9A)
- We modified Anthropic's RAG Cookbook to have Observability and unified API [link](https://github.com/Portkey-AI/gateway/blob/main/cookbook/use-cases/Contextual%20Embeddings%20Guide%20Anthropic%2C%20Cohere%2C%20Voyage.ipynb)
- End-to-End Guide for Using Vercel with Portkey [link](https://github.com/Portkey-AI/gateway/tree/main/cookbook/integrations/vercel)
- What is Automated Prompt Engineering? [link](https://portkey.ai/blog/what-is-automated-prompt-engineering/)
- OpenAI's Prompt Caching: A Deep Dive [link](https://portkey.ai/blog/openais-prompt-caching-a-deep-dive/)
- The Complete Guide to Prompt Engineering [link](https://portkey.ai/blog/the-complete-guide-to-prompt-engineering/)
- Multi-Agent AI Systems: OpenAI Swarm [link](https://portkey.ai/blog/multi-agent-ai-systems-openai-swarm/)
- The Developer's Guide to Opentelemetry [link](https://portkey.ai/blog/the-developers-guide-to-opentelemetry-a-real-time-journey-into-observability/)

More awesome content [here](https://portkey.ai/blog).

</Update>

<Update label="Fixes">
- Enhanced streaming transformer for Perplexity
- Fixed response transformation for Ollama
- ‚≠êÔ∏è Added missing logprob mapping for Azure OpenAI (Community Contribution by [Avishkar](https://www.linkedin.com/in/avishkar-gupta/)
- Token counting is now fixed for Vertex embeddings (we now count tokens instead of characters)
- Added default models for Gemini, Together AI, Fireworks AI.
  - Fireworks: `accounts/fireworks/models/llama-v3p1-405b-instruct`
  - Together AI: `meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo`
  - Gemini: `gemini-1.5-pro`
- Support for cross-region model IDs for Bedrock including cost calculations - https://github.com/Portkey-AI/gateway/pull/641 - Pricing for Bedrock cross-region model IDs- https://github.com/Portkey-AI/Winky/pull/278/files
- Fixed response transform for Ollama
- `anthropic-beta` and `anthropic-version` headers are now supported
- Fix for sending media files for Vertex AI & Gemini
- Support for additional headers from providers - https://github.com/Portkey-AI/portkey-python-sdk/pull/222/files
- API key is optional when using self-hosted Gateway with the Portkey SDK

</Update>

<Update label="Events">
- [**TED**](https://x.com/PortkeyAI/status/1847733473529999377): We co-sponsored the TED AI Hackathon!
- [**LLMs in Prod Dinner**](https://lu.ma/llms-in-prod-dinner): Singapore - We are organising a closed-door dinner with some of the leading tech executives in Singapore, on the preset of OpenAI Dev Day. [Link to Register](https://lu.ma/llms-in-prod-dinner)
</Update>

<Update label="News">
- OpenAI users were hitting usage limits earlier this month. Though, if you were on Portkey, [you should have been fine](https://x.com/PortkeyAI/status/1841172271076954588).
</Update>
---

*Found a bug or have a feature request? [Open an issue](https://github.com/Portkey-AI/gateway/issues) on our GitHub repository.*
