---
title: 'Vercel'
description: 'Integrate Portkey with Vercel AI SDK for production-ready and reliable AI apps'
---

Portkey natively integrates with the Vercel AI SDK to make your apps production-ready and reliable. Just import Portkey's Vercel package and use it as a provider in your Vercel AI app to enable all of Portkey features:

* Full-stack observability and tracing for all requests
* Interoperability across 250+ LLMS
* Built-in 50+ SOTA guardrails
* Simple & semantic caching to save costs & time
* Route requests conditionally and make them robust with fallbacks, load-balancing, automatic retries, and more
* Continuous improvement based on user feedback

## Getting Started

### 1. Installation

```bash
npm install @portkey-ai/vercel-provider
```

### 2. Import & Configure Portkey Object

[Sign up for Portkey](https://portkey.ai) and get your API key, and configure Portkey provider in your Vercel app:

```typescript
import { createPortkey } from '@portkey-ai/vercel-provider';

const portkeyConfig = {
      "provider": "openai", // Choose your provider (e.g., 'anthropic')
      "api_key": "OPENAI_API_KEY",
      "model": "gpt-4o" // Select from 250+ models
};

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});
```

<Info>
Portkey's configs are a powerful way to manage & govern your app's behaviour. Learn more about Configs [here](../../product/ai-gateway/configs).
</Info>

## Using Vercel Functions

Portkey provider works with all of Vercel functions `generateText`, `streamText`, `generateObject`, `streamObject`.

Here's how to use them with Portkey:

<CodeGroup>

```typescript generateText
import { createPortkey } from '@portkey-ai/vercel-provider';
import { generateText } from 'ai';

const portkeyConfig = {
      "provider": "openai", // Choose your provider (e.g., 'anthropic')
      "api_key": "OPENAI_API_KEY",
      "model": "gpt-4o" // Select from 250+ models
};

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const { text } = await generateText({
  model: portkey.chatModel(''),
  prompt: 'What is Portkey?',
});

console.log(text);
```

```typescript streamText
import { createPortkey } from '@portkey-ai/vercel-provider';
import { streamText } from 'ai';

const portkeyConfig = {
      "provider": "openai", // Choose your provider (e.g., 'anthropic')
      "api_key": "OPENAI_API_KEY",
      "model": "gpt-4o" // Select from 250+ models
};

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const result = await streamText({
  model: portkey('gpt-4-turbo'),
  prompt: 'Invent a new holiday and describe its traditions.',
});

for await (const chunk of result) {
  console.log(chunk);
}
```

```typescript generateObject
import { createPortkey } from '@portkey-ai/vercel-provider';
import { generateObject } from 'ai';
import { z } from 'zod';

const portkeyConfig = {
      "provider": "openai", // Choose your provider (e.g., 'anthropic')
      "api_key": "OPENAI_API_KEY",
      "model": "gpt-4o" // Select from 250+ models
};

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const { object } = await generateObject({
  model: portkey('gpt-4-turbo'),
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.string()),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a lasagna recipe.',
});

console.log(JSON.stringify(object, null, 2));
```

```typescript streamObject
import { createPortkey } from '@portkey-ai/vercel-provider';
import { streamObject } from 'ai';
import { z } from 'zod';

const portkeyConfig = {
      "provider": "openai", // Choose your provider (e.g., 'anthropic')
      "api_key": "OPENAI_API_KEY",
      "model": "gpt-4o" // Select from 250+ models
};

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const result = await streamObject({
  model: portkey('gpt-4-turbo'),
  schema: z.object({
    story: z.object({
      title: z.string(),
      characters: z.array(z.string()),
      plot: z.array(z.string()),
    }),
  }),
  prompt: 'Create a short story about time travel.',
});

for await (const chunk of result) {
  console.log(JSON.stringify(chunk, null, 2));
}
```

</CodeGroup>

## Portkey Features

Portkey Helps you make your Vercel app more robust and reliable. The portkey config is modular way to make it work for you in whatever way you want. 

### Interoperability

Portkey allows you to easily switch between 250+ AI models by simply changing the model name in your configuration. This flexibility enables you to adapt to the evolving AI landscape without significant code changes.

<CodeGroup>

```typescript Switch from OpenAI to Anthropic
// OpenAI configuration
const portkeyConfig = {
      "provider": "openai",
      "api_key": "OPENAI_API_KEY",
      "model": "gpt-4-turbo"
};

// Anthropic configuration
const portkeyConfig = {
      "provider": "anthropic",
      "api_key": "Anthropic_API_KEY",
      "model": "claude-3-5-sonnet-20240620"
};
```

</CodeGroup>

### Observability

Portkey's OpenTelemetry-compliant observability suite gives you complete control over all your requests. And Portkey's analytics dashboards provide **40**+ key insights you're looking for including cost, tokens, latency, etc. Fast.

<Frame>
  <img src="/images/product/dashboard.png" alt="Portkey's Observability Dashboard" />
</Frame>

### Reliability

Portkey enhances the robustness of your AI applications with built-in features such as [Caching](../../product/ai-gateway/cache-simple-and-semantic), [Fallback](../../product/ai-gateway/fallbacks) mechanisms, [Load balancing](../../product/ai-gateway/load-balancing), [Conditional routing](../../product/ai-gateway/conditional-routing), [Request timeouts](../../product/ai-gateway/request-timeouts), etc. 

Here is how you can modify your config to include the following Portkey features:

<CodeGroup>

```typescript Fallback
import { createPortkey } from '@portkey-ai/vercel-provider';
import { generateText } from 'ai';

const portkeyConfig =  {
	"strategy": {
		"mode": "fallback"
	},
	"targets": [
		{ "virtual_key":"anthropic-key" },
		{ "virtual_key":"aws-bedrock-key" }
	]
}

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const { text } = await generateText({
  model: portkey.chatModel(''),
  prompt: 'What is Portkey?',
});

console.log(text);
```

```typescript Caching
import { createPortkey } from '@portkey-ai/vercel-provider';
import { generateText } from 'ai';

const portkeyConfig = { "cache": { "mode": "semantic" } }

const portkey = createPortkey({
  apiKey: 'YOUR_PORTKEY_API_KEY',
  config: portkeyConfig,
});

const { text } = await generateText({
  model: portkey.chatModel(''),
  prompt: 'What is Portkey?',
});

console.log(text);
```

```typescript Conditional routing
const portkey_config =  {
	"strategy": {
		"mode": "conditional",
		"conditions": [
			...conditions
		],
		"default": "target_1"
	},
	"targets": [
		{
			"name": "target_1",
			"virtual_key":"xx"
		},
		{
			"name": "target_2",
			"virtual_key":"yy"
		}
	]
}
```

</CodeGroup>

Learn more about Portkey's AI gateway features in detail [here](../../product/ai-gateway/).

### Guardrails

Portkey Guardrails allow you to enforce LLM behavior in real-time, verifying both inputs and outputs against specified checks. 

You can create Guardrail checks in UI and then pass them in your Portkey Configs with before request or after request hooks.

[Read more about Guardrails here](../../product/guardrails).

## Portkey Config

Many of these features are driven by Portkey's Config architecture. The Portkey app simplifies creating, managing, and versioning your Configs.

For more information on using these features and setting up your Config, please refer to the [Portkey documentation](https://docs.portkey.ai).
