---
title: "October '24"
---

# üéÉ ü™î Portkey in October

October was packed with treats (no tricks!) for Portkey. As we celebrate Halloween and Diwali, we're lighting up your AI infrastructure with some exciting updates. Let's dive in!

---

## Highlights

- **Guardrails is now GA**: Our guardrails feature is now generally available, helping you enforce LLM behavior in realtime. [(*Docs*)](/product/guardrails)
- **Enterprise Updates**: Refreshed our [enterprise offering](https://portkey.ai/docs/product/enterprise-offering) and [welcomed](https://x.com/PortkeyAI/status/1841292805454643393) one of the world's largest tech companies to the Portkey family.
- **Featured in Media**: Check out our why we're building *DevOps for AI* in the [People+AI Newsletter](https://sreeramsridhar.substack.com/p/building-the-devops-for-ai) and our CEO's interview on [Pulse2](https://pulse2.com/portkey-profile-rohit-agarwal-interview/)
- **Anthropic Prompt Caching in Playground**: For Anthropic models, you can now enable any message to be cached
- **Portkey Tops Agent Tooling Framework**: Portkey provides 11 critical capabilities to put agents in production. https://x.com/PortkeyAI/status/1851596076488479001
---

## New Features

#### Auth Upgrades
- AWS Assume Role Support for Bedrock. [(*Docs*)](/product/ai-gateway/virtual-keys/bedrock-amazon-assumed-role)

#### APIs
- [Resend a User Invite](/api-reference/admin-api/control-plane/admin/user-invites/resend-a-user-invite)

We also added API specs for:
<CardGroup cols={3}>
<Card horizontal="true" title="Prompt Completions API" href="/api-reference/inference-api/prompts/prompt-completion"></Card>
<Card horizontal="true" title="Prompt Render API" href="/api-reference/inference-api/prompts/prompt-render"></Card>
<Card horizontal="true" title="Inserting Logs API" href="/api-reference/admin-api/data-plane/logs/insert-a-log"></Card>
</CardGroup>

#### Providers

<CardGroup cols={4}>
<Card title="Lemonfox" href="/integrations/llms/lemon-fox"></Card>
<Card title="Lambda Labs" href="/integrations/llms/lambda"></Card>
<Card title="Dashscope" href="/integrations/llms/dashscope"></Card>
<Card title="Upstage" href="/integrations/llms/upstage"></Card>
<Card title="Github"></Card>
<Card title="vLLM" href="/integrations/llms/vllm"></Card>
</CardGroup>

#### Models
- [Stable Diffusion v3](/api-reference/inference-api/images/create-image) (across [Stability AI](/integrations/llms/stability-ai), [Fireworks](/integrations/llms/fireworks), [AWS Bedrock](/integrations/llms/aws-bedrock), and [Segmind](/integrations/llms/segmind))
- Llama 3.2 (across [Fireworks](/integrations/llms/fireworks), [AWS Bedrock](/integrations/llms/aws-bedrock), [Groq](/integrations/llms/groq), [Together AI](/integrations/llms/together-ai))
- [Vertex English & Multilingual Embeddings](/integrations/llms/vertex-ai#text-embedding-models)
- [Imagen on Google Vertex](https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview) for image generation. [(*Docs*)](/api-reference/inference-api/images/create-image)

#### Improvements
- OpenAI's `max_completion_tokens` is now supported across all providers. (this means you can send either the `max_completion_tokens` or `max_tokens` param in your requests and Portkey will handle both correctly.)
- Updated cost calculations for cached responses for both OpenAI & Azure OpenAI
- JSON Mode Gemini models
- Gemini Controlled Generations is now supported on Portkey (alnog with support for Pydantic!)
- We've integrated Bedrock's Converse API for all `/chat/completions` requests

#### Guardrails
<CardGroup cols={1}>
<Card title="Lowercase Detection">Check if the given string is lowercase or not.</Card>
<Card title="Custom Webhooks">Along with the Webhook information, you can now send any custom metadata along with your request</Card>
</CardGroup>

**Portkey's LLM Guardrails are also updated:**
You can do `PII Detection`, `Language Detection`, `Moderation`, and `Gibberish Detection`.


#### SDKs
**C#**: See how you can integrate Portkey in your .NET app easily using the OpenAI library and get advanced monitoring, routing, and enterprise features. [(*Docs*)](/api-reference/inference-api/sdks/c-sharp)

---

## New Integrations
<CardGroup cols={1}>
<Card title="LibreChat for Portkey" href="https://github.com/timmanik/librechat-for-portkey">[Tim](https://www.linkedin.com/in/tim-manik/) wrote up a way to send unique user IDs from LibreChat back to Portkey. Very useful if you‚Äôre a system admin, and you‚Äôre looking to track the costs/user on a centralized instance of LibreChat.</Card>
<Card title="MindsDB" href="/integrations/libraries/mindsdb">Connect your databases, vector stores, and apps to 250+ LLMs with enterprise-grade monitoring and reliability built-in.</Card>
<Card tile="OpenWebUI" href="/integrations/libraries/openwebui">Portkey is the only plugin you‚Äôll need for your model management, cost tracking, observability, metadata logging, and more for your Open WebUI instance.</Card>
<Card title="ToolJet" href="/integrations/libraries/tooljet">Add AI-powered capabilities such as chat completions and automations into your ToolJet apps easily</Card>
</CardGroup>

---

## New Resources

#### 2-Min Guides
- [Guide to Prompt Caching](https://x.com/PortkeyAI/status/1843209780627997089)
- [Building Prod-Ready Apps with Vercel](https://x.com/PortkeyAI/status/1844675148609204615)
- [OpenAI Swarm Cheat Sheet](https://x.com/jumbld/status/1846909380354064526)

#### Longer Guides
- [How to Build Multi-Agent AI Systems with OpenAI Swarm & Secure Them Using Portkey](https://www.youtube.com/watch?v=9qLkAEJol9A)
- [We modified Anthropic's RAG Cookbook to have Observability and unified API](https://github.com/Portkey-AI/gateway/blob/main/cookbook/use-cases/Contextual%20Embeddings%20Guide%20Anthropic%2C%20Cohere%2C%20Voyage.ipynb)
- [End-to-End Guide for Using Vercel with Portkey](https://github.com/Portkey-AI/gateway/tree/main/cookbook/integrations/vercel)
- [What is Automated Prompt Engineering?](https://portkey.ai/blog/what-is-automated-prompt-engineering/)
- [OpenAI's Prompt Caching: A Deep Dive](https://portkey.ai/blog/openais-prompt-caching-a-deep-dive/)
- [The Complete Guide to Prompt Engineering](https://portkey.ai/blog/the-complete-guide-to-prompt-engineering/)
- [Multi-Agent AI Systems: OpenAI Swarm](https://portkey.ai/blog/multi-agent-ai-systems-openai-swarm/)
- [The Developer's Guide to Opentelemetry](https://portkey.ai/blog/the-developers-guide-to-opentelemetry-a-real-time-journey-into-observability/)

More awesome content [here](https://portkey.ai/blog).

---

## Notable Fixes
- Enhanced streaming transformer for Perplexity
- Fixed response transformation for Ollama
- ‚≠êÔ∏è Added missing logprob mapping for Azure OpenAI (Community Contribution by [Avishkar](https://www.linkedin.com/in/avishkar-gupta/)
- Token counting is now fixed for Vertex embeddings (we now count tokens instead of characters)
- Added default models for Gemini, Together AI, Fireworks AI.
  - Fireworks: `accounts/fireworks/models/llama-v3p1-405b-instruct`
  - Together AI: `meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo`
  - Gemini: `gemini-1.5-pro`
- Support for cross-region model IDs for Bedrock including cost calculations - https://github.com/Portkey-AI/gateway/pull/641 - Pricing for Bedrock cross-region model IDs- https://github.com/Portkey-AI/Winky/pull/278/files
- Fixed response transform for Ollama
- `anthropic-beta` and `anthropic-version` headers are now supported
- Fix for sending media files for Vertex AI & Gemini
- Support for additional headers from providers - https://github.com/Portkey-AI/portkey-python-sdk/pull/222/files
- API key is optional when using self-hosted Gateway with the Portkey SDK

---

## New Downtimes
- OpenAI usage limits - https://x.com/PortkeyAI/status/1841172271076954588

---

## New Events
- TED AI Hackathon - https://x.com/PortkeyAI/status/1847733473529999377


## Coming Soon
Stay tuned for more exciting updates! Follow us on [Twitter/X](https://x.com/PortkeyAI) for real-time announcements.

---

*Found a bug or have a feature request? [Open an issue](https://github.com/Portkey-AI/gateway/issues) on our GitHub repository.*
