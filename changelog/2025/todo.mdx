To write standalone docs or make updates to docs for the following or do the GTM for the following:
- update openapi spec for chat completions with reasoning_effort and more stuff
- custom webhooks mutation capability
- make a simple guide for enterprise users - this is what would be different for you. for owners, org admins, workspace manager, and workspace members, and then people with service keys
- google search tool
- fine-tuning, files, batches- 
- parnter & pro guardrails have configurable timeouts
- conditional routing can now be done using request params
- cache status filter in the cache page
- New docs for logs export and replace the existing ones
- In the instrumentation docs, add a note for how to do it on self-hosted Portkey

-----------

Feb changelog:

Here are the updates for this month:
Gateway
- add support for reasoning_effort param in openai
- Acuvity portkey guardrail
- the gateway now by default caches your vertex generated token
- suport for upload file endpoint on azure, openai, vertex, bedrock, fireworks
- add support for google search tool on the gateway (google_search is a separate tool from google_search_retrieval and the newer models like gemini2.0-flash don't support google_search_retrieval
- Handle inconsistent usage object in vertex and google streaming responses - vertex ai returns an empty usage object in the very first streaming chunk. Expected behaviour: only the last chunk should contain a usage object
- handle tool_calls mapping in gemini responses when there is one part tool call and one part text
- Custom webhooks on the Gateway now also let you mutate the request/response bodies. Your webhook just needs to send a new transformedData object along with the verdict - we'll look for request or response bodies in there and if they exist, we will override the existing request or response body with the transformedData that your webhook sends - <this is an amazing feature />
- All Partner & Pro Guardrails now have configurable timeouts
- Fix: Azure OpenAI streaming responses now include the last chunk with `stream_options` that has the `usage`
- Unified Fine-tuning, Files, Batches API for OpenAI, Azure OpenAI, Google Vertex AI, AWS Bedrock, Fireworks AI. Batches API will also work with any provider that Portkey supports
- You can now conditionally route based on any of your request params and also modify the param value to any random string and define that at the conditional router level
- Multimodal requests on Vertex can now use 'webm'
Cookbooks
- [Track Costs Using Metadata](/guides/use-cases/track-costs-using-metadata)
- [Deepseek R1](/guides/use-cases/deepseek-r1)
- Prompt Engineering Cookbooks: https://portkey.ai/docs/guides/prompts, building an ultimate AI SDR: https://portkey.ai/docs/guides/prompts/ultimate-ai-sdr
New feature:
- PII Redaction 
- Grounding for Vertex & Gemini
- Secure, observe, and govern your LLM interactions on Zed for your entire team
- Integration with AnythingLLM
- Integration with JanHQ
- Portkey is now on Azure Marketplace: https://azuremarketplace.microsoft.com/en-in/marketplace/apps/portkey.enterprise-saas?tab=Overview
- Auto instrumentation for CrewAI & LangGraph and still retain all of Portkey features for interoperability, metering, governance, routing, and more.
Docs improvement: 
- Added all the erorrs that originate from Portkey: https://portkey.ai/docs/api-reference/inference-api/error-codes
- Add Default Configs to API Keys
- Prompt Engineering Studio is the main highlight launch of this month
- Change member roles in UI
- Create User key from UI
- Allow multiple Owners in a single org - done through UI
New models
- o3 models from OpenAI
- gemini 2 flash thinking
- Claude 3.7 Sonnet
- openai gpt 4.5
Community
- We attended the AI Engineering Summit in NYC - https://x.com/aiDotEngineer/status/1886419969526919564
- We chatted with swyx and Alessio from the Latent Space podcast - https://www.youtube.com/watch?v=-rSbvS0qLqY | Here are the takeaways: https://x.com/PortkeyAI/status/1887495934789230759

User Stories
- <img src="/images/changelog/testimonial-analytical-monk.png" />
- "Describing Portkey as merely useful would be an understatement; it's a must-have." - @AManInTech <img src="/images/changelog/aman-testimonial.jpeg" />

Our Stories:
- The State of AI FinOps 2025: Key Insights from FinOps Foundation's Latest Report- https://portkey.ai/blog/the-state-of-ai-finops-2025-key-insights-from-finops-foundations-latest-report/

January was a record-breaking month for Portkey. We saw unprecedented enterprise adoption, closing more enterprise deals in January alone than in the final months of 2024 combined.

What's even more thrilling is the scale of AI adoption we're enabling: 

- processed ~250M LLM calls in just the past week
- ~60% calls have fallbacks configured
- ~39% calls have load balancing or A/B Testing enabled
- a large percentage have atleast one runtime guardrail check

Cheers to our amazing team making it possible. Here‚Äôs to even bigger wins ahead! üçªüî•



Beyon all of these, some updates that are unique to Enterprises:
Updated cache implementation to avoid redundant Redis calls to improve overall performance.

SDK updates:
We now have support for running our SDK in the Browser.
We have also enabled Cross-Origin access for our APIs.

Now, shout out to Community Contributors:
- https://github.com/ethanknights
- Matthias Endler, https://github.com/mre

-------------------