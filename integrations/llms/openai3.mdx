---
title: 'OpenAI'
description: Portkey supports OpenAI's chat completions, embeddings, audio, image, fine-tuning, batches, and more APIs.
---

<ResponseField name="Provider Slug" type="openai">
[Latest Pricing](https://models.portkey.ai/providers/openai) | [API Status](https://status.portkey.ai/) | [Supported Endpoints](/api-reference/inference-api/supported-providers)
</ResponseField>

## Integrate
Just paste your OpenAI API Key from [here](https://platform.openai.com/account/api-keys) to [Portkey](https://app.portkey.ai/virtual-keys) to create your Virtual Key.
<Frame>
<img src="/images/integrations/openai/virtual-key-2.png" width="350" />
</Frame>
<Info>
Optional
- Add your OpenAI organization and project ID details: ([Docs](#openai-projects-and-organizations))
- Directly use OpenAI API key without the  Virtual Key: ([Docs](/api-reference/inference-api/headers#1-provider-slug-auth))
- Create a short-lived virtual key OR one with usage/rate limits: ([Docs](/product/ai-gateway/virtual-keys))
</Info>

## Sample Request
Portkey is a drop-in replacement for OpenAI. You can make request using the official OpenAI or Portkey SDKs.
<Note>
Popular libraries & agent frameworks like LangChain, CrewAI, AutoGen, etc. are [also supported](#popular-libraries).
All Azure OpenAI models & endpoints are [also supported](/integrations/llms/azure-openai)
</Note>
<CodeGroup>

```ts NodeJS
import Portkey from 'portkey-ai';

const client = new Portkey({
  apiKey: 'PORTKEY_API_KEY',
  virtualKey: 'PROVIDER_VIRTUAL_KEY'
});

async function main() {
  const response = await client.chat.completions.create({
    messages: [{ role: "user", content: "Bob the builder.." }],
    model: "gpt-4o",
  });

  console.log(response.choices[0].message.content);
}

main();
```
```py Python
from portkey_ai import Portkey

client = Portkey(
  api_key = "PORTKEY_API_KEY",
  virtual_key = "PROVIDER_VIRTUAL_KEY"
)

response = client.chat.completions.create(
  model="gpt-4o",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ]
)

print(response.choices[0].message)
```
```sh cURL
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-virtual-key: $PORTKEY_PROVIDER_VIRTUAL_KEY" \
  -d '{
    "model": "gpt-4o",
    "messages": [
      { "role": "user", "content": "Hello!" }
    ]
  }'
```
```py OpenAI Python SDK
from openai import OpenAI
from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL

client = OpenAI(
    api_key="xx",
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        api_key="PORTKEY_API_KEY",
        virtual_key="OPENAI_VIRTUAL_KEY"
    )
)

completion = client.chat.completions.create(
  model="gpt-4o",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ]
)

print(completion.choices[0].message)
```
```ts OpenAI NodeJS SDK
import OpenAI from 'openai';
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai'

const openai = new OpenAI({
  apiKey: 'xx',
  baseURL: PORTKEY_GATEWAY_URL,
  defaultHeaders: createHeaders({
    apiKey: "PORTKEY_API_KEY",
    virtualKey: "OPENAI_VIRTUAL_KEY"
  })
});

async function main() {
  const completion = await openai.chat.completions.create({
    messages: [{ role: 'user', content: 'Say this is a test' }],
    model: 'gpt-4o',
  });

  console.log(chatCompletion.choices);
}

main();
```
</CodeGroup>

## Local Setup
If you do not want to use Portkey's hosted API, you can also run Portkey locally:
<AccordionGroup>
<Accordion title="Open Source (npm or docker)">
Portkey runs on our popular [open source Gateway](https://git.new/portkey). You can spin it up locally to make requests without sending them to the Portkey API.
<CodeGroup>
```sh Install the Gateway
npx @portkey-ai/gateway
```

```sh Docker Image
npx @portkey-ai/gateway
```
</CodeGroup>
| Your Gateway is running on http://localhost:8080/v1 ðŸš€ | |
| - | - |

Then, just change the `baseURL` to the local Gateway URL, and make requests:
<CodeGroup>
```ts NodeJS
import Portkey from 'portkey-ai';

const client = new Portkey({
  baseUrl: 'http://localhost:8080/v1',
  apiKey: 'PORTKEY_API_KEY',
  virtualKey: 'PROVIDER_VIRTUAL_KEY'
});

async function main() {
  const response = await client.chat.completions.create({
    messages: [{ role: "user", content: "Bob the builder.." }],
    model: "gpt-4o",
  });

  console.log(response.choices[0].message.content);
}

main();
```
```py Python
from portkey_ai import Portkey

client = Portkey(
  base_url = 'http://localhost:8080/v1',
  api_key = "PORTKEY_API_KEY",
  virtual_key = "PROVIDER_VIRTUAL_KEY"
)

response = client.chat.completions.create(
  model="gpt-4o",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ]
)

print(response.choices[0].message)
```
```sh cURL
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-virtual-key: $PORTKEY_PROVIDER_VIRTUAL_KEY" \
  -d '{
    "model": "gpt-4o",
    "messages": [
      { "role": "user", "content": "Hello!" }
    ]
  }'
```
```py OpenAI Python SDK
from openai import OpenAI
from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL

client = OpenAI(
    api_key="xx",
    base_url="https://localhost:8080/v1",
    default_headers=createHeaders(
        api_key="PORTKEY_API_KEY",
        virtual_key="OPENAI_VIRTUAL_KEY"
    )
)

completion = client.chat.completions.create(
  model="gpt-4o",
  messages=[
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ]
)

print(completion.choices[0].message)
```
```ts OpenAI NodeJS SDK
import OpenAI from 'openai';
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai'

const openai = new OpenAI({
  apiKey: 'xx',
  baseURL: 'https://localhost:8080/v1',
  defaultHeaders: createHeaders({
    apiKey: "PORTKEY_API_KEY",
    virtualKey: "OPENAI_VIRTUAL_KEY"
  })
});

async function main() {
  const completion = await openai.chat.completions.create({
    messages: [{ role: 'user', content: 'Say this is a test' }],
    model: 'gpt-4o',
  });

  console.log(chatCompletion.choices);
}

main();
```
</CodeGroup>

</Accordion>
<Accordion title="On-Prem Deployment (AWS, GCP, Azure)">
    Portkey's data & control planes can be fully deployed on-prem with the Enterprise license.

    [More details here](http://localhost:3000/product/enterprise-offering/private-cloud-deployments)
</Accordion>
</AccordionGroup>

---

## Integration Overview

### OpenAI Endpoints & Capabilities

Portkey works with *all* of OpenAI's endpoints and supports all OpenAI capabilities like prompt caching, structured outputs, and more. Find examples for each below:
<AccordionGroup>
<Accordion title="Tool Calling (Function Calling)">
<CodeGroup>
```javascript Node.js
let tools = [{
    type: "function",
    function: {
        name: "getWeather",
        description: "Get the current weather",
        parameters: {
            type: "object",
            properties: {
                location: { type: "string", description: "City and state" },
                unit: { type: "string", enum: ["celsius", "fahrenheit"] }
            },
            required: ["location"]
        }
    }
}];

let response = await portkey.chat.completions.create({
    model: "gpt-4o",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        { role: "user", content: "What's the weather like in Delhi - respond in JSON" }
    ],
    tools,
    tool_choice: "auto",
});

console.log(response.choices[0].finish_reason);
```

```python Python
tools = [{
    "type": "function",
    "function": {
        "name": "getWeather",
        "description": "Get the current weather",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City and state"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location"]
        }
    }
}]

response = portkey.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What's the weather like in Delhi - respond in JSON"}
    ],
    tools=tools,
    tool_choice="auto"
)

print(response.choices[0].finish_reason)
```

```curl REST
curl -X POST "https://api.portkey.ai/v1/chat/completions" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -d '{
       "model": "gpt-4o",
       "messages": [
         {"role": "system", "content": "You are a helpful assistant."},
         {"role": "user", "content": "What'\''s the weather like in Delhi - respond in JSON"}
       ],
       "tools": [{
         "type": "function",
         "function": {
           "name": "getWeather",
           "description": "Get the current weather",
           "parameters": {
             "type": "object",
             "properties": {
               "location": {"type": "string", "description": "City and state"},
               "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
             },
             "required": ["location"]
           }
         }
       }],
       "tool_choice": "auto"
     }'
```
</CodeGroup>
</Accordion>

<Accordion title="Streaming">
</Accordion>

<Accordion title="Vision">

Process images alongside text using OpenAI's vision capabilities:

<CodeGroup>
```python Python
response = portkey.chat.completions.create(
    model="gpt-4-vision-preview",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "What's in this image?"},
                {
                    "type": "image_url",
                    "image_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                },
            ],
        }
    ],
    max_tokens=300,
)

print(response)
```

```javascript Node.js
const response = await portkey.chat.completions.create({
  model: "gpt-4-vision-preview",
  messages: [
    {
      role: "user",
      content: [
        { type: "text", text: "What's in this image?" },
        {
          type: "image_url",
          image_url: "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
        },
      ],
    },
  ],
  max_tokens: 300,
});

console.log(response);
```

```curl REST
curl -X POST "https://api.portkey.ai/v1/chat/completions" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -d '{
       "model": "gpt-4-vision-preview",
       "messages": [
         {
           "role": "user",
           "content": [
             {"type": "text", "text": "What'\''s in this image?"},
             {
               "type": "image_url",
               "image_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
             }
           ]
         }
       ],
       "max_tokens": 300
     }'
```
</CodeGroup>

</Accordion>

<Accordion title="Embeddings">

Generate embeddings for text using OpenAI's embedding models:

<CodeGroup>
```python Python
response = portkey.embeddings.create(
    input="Your text string goes here",
    model="text-embedding-3-small"
)

print(response.data[0].embedding)
```

```javascript Node.js
const response = await portkey.embeddings.create({
  input: "Your text string goes here",
  model: "text-embedding-3-small"
});

console.log(response.data[0].embedding);
```

```curl REST
curl -X POST "https://api.portkey.ai/v1/embeddings" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -d '{
       "input": "Your text string goes here",
       "model": "text-embedding-3-small"
     }'
```
</CodeGroup>
</Accordion>

<Accordion title="Transcription and Translation">

Portkey supports both `Transcription` and `Translation` methods for STT models:

<CodeGroup>
```python Python
audio_file= open("/path/to/file.mp3", "rb")

# Transcription
transcription = portkey.audio.transcriptions.create(
  model="whisper-1",
  file=audio_file
)
print(transcription.text)

# Translation
translation = portkey.audio.translations.create(
  model="whisper-1",
  file=audio_file
)
print(translation.text)
```

```javascript Node.js
import fs from "fs";

// Transcription
async function transcribe() {
  const transcription = await portkey.audio.transcriptions.create({
    file: fs.createReadStream("/path/to/file.mp3"),
    model: "whisper-1",
  });
  console.log(transcription.text);
}
transcribe();

// Translation
async function translate() {
    const translation = await portkey.audio.translations.create({
        file: fs.createReadStream("/path/to/file.mp3"),
        model: "whisper-1",
    });
    console.log(translation.text);
}
translate();
```

```curl REST
# Transcription
curl -X POST "https://api.portkey.ai/v1/audio/transcriptions" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@/path/to/file.mp3" \
     -F "model=whisper-1"

# Translation
curl -X POST "https://api.portkey.ai/v1/audio/translations" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@/path/to/file.mp3" \
     -F "model=whisper-1"
```
</CodeGroup>

</Accordion>

<Accordion title="Text to Speech">

Convert text to speech using OpenAI's TTS models:

<CodeGroup>
```python Python
from pathlib import Path

speech_file_path = Path(__file__).parent / "speech.mp3"
response = portkey.audio.speech.create(
  model="tts-1",
  voice="alloy",
  input="Today is a wonderful day to build something people love!"
)

with open(speech_file_path, "wb") as f:
    f.write(response.content)
```

```javascript Node.js
import path from 'path';
import fs from 'fs';

const speechFile = path.resolve("./speech.mp3");

async function main() {
  const mp3 = await portkey.audio.speech.createCertainly! I'll continue with the Text to Speech section and then move on to the additional features and sections:

```javascript Node.js
({
    model: "tts-1",
    voice: "alloy",
    input: "Today is a wonderful day to build something people love!",
  });
  const buffer = Buffer.from(await mp3.arrayBuffer());
  await fs.promises.writeFile(speechFile, buffer);
}

main();
```

```curl REST
curl -X POST "https://api.portkey.ai/v1/audio/speech" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -H "Content-Type: application/json" \
     -d '{
       "model": "tts-1",
       "voice": "alloy",
       "input": "Today is a wonderful day to build something people love!"
     }' \
     --output speech.mp3
```
</CodeGroup>

</Accordion>

<Accordion title="Prompt Caching">

Implement prompt caching to improve performance and reduce costs:

<Card title="Prompt Caching Guide" icon="bolt" href="/docs/integrations/llms/openai/prompt-caching-openai">
  Learn how to implement prompt caching for OpenAI models with Portkey.
</Card>

</Accordion>

<Accordion title="Realtime API">
</Accordion>

<Accordion title="Structured Outputs">

Use structured outputs for more consistent and parseable responses:

<Card title="Structured Outputs Guide" icon="code" href="/docs/integrations/llms/openai/structured-outputs">
  Discover how to use structured outputs with OpenAI models in Portkey.
</Card>

</Accordion>

<Accordion title="Image Generation">
</Accordion>
</AccordionGroup>


### Portkey Features

<AccordionGroup>

<Accordion title="Track End-User IDs">

Portkey allows you to track user IDs passed with the user parameter in OpenAI requests, enabling you to monitor user-level costs, requests, and more:

<CodeGroup>
```python Python
response = portkey.chat.completions.create(
  model="gpt-4o",
  messages=[{"role": "user", "content": "Say this is a test"}],
  user="user_123456"
)
```

```javascript Node.js
const chatCompletion = await portkey.chat.completions.create({
  messages: [{ role: "user", content: "Say this is a test" }],
  model: "gpt-4o",
  user: "user_12345",
});
```

```curl REST
curl -X POST "https://api.portkey.ai/v1/chat/completions" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -d '{
       "model": "gpt-4o",
       "messages": [{"role": "user", "content": "Say this is a test"}],
       "user": "user_123456"
     }'
```
</CodeGroup>

When you include the user parameter in your requests, Portkey logs will display the associated user ID, as shown in the image below:

<img src="/images/llms/logs.png" alt="Portkey Logs with User ID" />

In addition to the `user` parameter, Portkey allows you to send arbitrary custom metadata with your requests. This powerful feature enables you to associate additional context or information with each request, which can be useful for analysis, debugging, or other custom use cases.

<CardGroup cols={1}>
  <Card title="Learn More About Metadata" icon="tags" href="/docs/product/observability/metadata">
    Explore how to use custom metadata to enhance your request tracking and analysis.
  </Card>
</CardGroup>

</Accordion>

<Accordion title="Setup Fallbacks & Loadbalancer">

Here's a simplified version of how to use Portkey's Gateway Configuration:

<Steps>
  <Step title="Create a Gateway Configuration" titleSize="h3">
    You can create a Gateway configuration using the Portkey Config Dashboard or by writing a JSON configuration in your code. In this example, requests are routed based on the user's subscription plan (paid or free).

    ```json
    config = {
      "strategy": {
        "mode": "conditional",
        "conditions": [
          {
            "query": { "metadata.user_plan": { "$eq": "paid" } },
            "then": "gpt4o"
          },
          {
            "query": { "metadata.user_plan": { "$eq": "free" } },
            "then": "gpt-3.5"
          }
        ],
        "default": "base-gpt4"
      },
      "targets": [
        {
          "name": "gpt4o",
          "virtual_key": "xx"
        },
        {
          "name": "gpt-3.5",
          "virtual_key": "yy"
        }
      ]
    }
    ```
  </Step>

  <Step title="Process Requests" titleSize="h3">
    When a user makes a request, it will pass through Portkey's AI Gateway. Based on the configuration, the Gateway routes the request according to the user's metadata.
    <img src="/images/llms/conditional-routing.png" alt="Conditional Routing Diagram" />
  </Step>

  <Step title="Set Up the Portkey Client" titleSize="h3">
    Pass the Gateway configuration to your Portkey client. You can either use the config object or the Config ID from Portkey's hosted version.

    <CodeGroup>
    ```python Python
    from portkey_ai import Portkey

    portkey = Portkey(
        api_key="PORTKEY_API_KEY",
        virtual_key="VIRTUAL_KEY",
        config=portkey_config
    )
    ```

    ```javascript Node.js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
      apiKey: "PORTKEY_API_KEY",
      virtualKey: "VIRTUAL_KEY",
      config: portkeyConfig
    })
    ```
    </CodeGroup>
  </Step>
</Steps>

That's it! Portkey seamlessly allows you to make your AI app more robust using built-in gateway features. Learn more about advanced gateway features:

<CardGroup cols={2}>
  <Card title="Load Balancing" icon="balance-scale" href="/docs/product/ai-gateway/load-balancing">
    Distribute requests across multiple targets based on defined weights.
  </Card>
  <Card title="Fallbacks" icon="life-ring" href="/docs/product/ai-gateway/fallbacks">
    Automatically switch to backup targets if the primary target fails.
  </Card>
  <Card title="Conditional Routing" icon="route" href="/docs/product/ai-gateway/conditional-routing">
    Route requests to different targets based on specified conditions.
  </Card>
  <Card title="Caching" icon="database" href="/docs/product/ai-gateway/caching">
    Enable caching of responses to improve performance and reduce costs.
  </Card>
</CardGroup>

</Accordion>

<Accordion title="Setup Guardrails">

Portkey's AI gateway enables you to enforce input/output checks on requests by applying custom hooks before and after processing. Protect your user's/company's data by using PII guardrails and many more available on Portkey Guardrails:

```json
{
	"virtual_key":"openai-xxx",
	"before_request_hooks": [{
		"id": "input-guardrail-id-xx"
	}],
	"after_request_hooks": [{
		"id": "output-guardrail-id-xx"
	}]
}
```

<Card title="Learn More About Guardrails" icon="shield-check" href="/docs/product/guardrails">
  Explore Portkey's guardrail features to enhance the security and reliability of your AI applications.
</Card>

</Accordion>

<Accordion title="Cache Requests">
</Accordion>

<Accordion title="Send Custom Metadata">
</Accordion>

<Accordion title="Send Custom Metadata">
</Accordion>

<Accordion title="Setup Rate Limits">
</Accordion>

<Accordion title="Create & Deploy Prompt Templates">
</Accordion>

</AccordionGroup>


### Popular Libraries

You can make your OpenAI integrations with popular libraries also production-ready and reliable with native integrations.

<AccordionGroup>
<Accordion title="OpenAI with Langchain">
</Accordion>
<Accordion title="OpenAI with LangGraph">
</Accordion>
<Accordion title="OpenAI with Llamaindes">
</Accordion>
<Accordion title="OpenAI with CrewAI">
</Accordion>
<Accordion title="OpenAI with CrewAI">
</Accordion>
<Accordion title="OpenAI with Vercel">
</Accordion>
</AccordionGroup>

---

## Cookbooks

<CardGroup cols={2}>
    <Card title="Setup a fallback from OpenAI to Azure OpenAI" />
<Card title="A/B test your prompts" />
</CardGroup>

---

## Appendix

### OpenAI Projects & Organizations
<Accordion title="Managing OpenAI Orgs on Portkey">
Organization management is particularly useful if you belong to multiple organizations or are accessing projects through a legacy OpenAI user API key. Specifying the organization and project IDs also helps you maintain better control over your access rules, usage, and costs.

In Portkey, you can add your OpenAI Org & Project details by **Using Virtual Keys**, **Using Configs**, or **While Making a Request**.

<AccordionGroup>

<Accordion title="Using Virtual Keys">

When selecting OpenAI from the Virtual Key dropdown menu while creating a virtual key, Portkey displays optional fields for the organization ID and project ID alongside the API key field.
<Frame>
<img src="/images/integrations/openai/virtual-key-2.png" width="500" />
</Frame>
<Info>
Portkey takes budget management a step further than OpenAI. While OpenAI allows setting budget limits per project, Portkey enables you to set budget limits for each virtual key you create. For more information on budget limits, [refer to this documentation](/product/ai-gateway/virtual-keys/budget-limits)
</Info>

</Accordion>

<Accordion title="Using Configs">

You can also specify the organization and project details in your request config, either at the root level or within a specific target.

```json {3,4}
{
	"provider": "openai",
	"api_key": "OPENAI_API_KEY",
	"openai_organization": "org-xxxxxx",
	"openai_project": "proj_xxxxxxxx"
}
```

</Accordion>

<Accordion title="While Making a Request">

Pass OpenAI organization and project details directly when making a request:

<CodeGroup>
```python OpenAI Python
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

client = OpenAI(
    api_key="OPENAI_API_KEY",
    organization="org-xxxxxxxxxx",
    project="proj_xxxxxxxxx",
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        provider="openai",
        api_key="PORTKEY_API_KEY"
    )
)

chat_complete = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Say this is a test"}],
)

print(chat_complete.choices[0].message.content)
```

```js OpenAI NodeJS
import OpenAI from "openai";
import { PORTKEY_GATEWAY_URL, createHeaders } from "portkey-ai";

const openai = new OpenAI({
  apiKey: "OPENAI_API_KEY",
  organization: "org-xxxxxx",
  project: "proj_xxxxxxx",
  baseURL: PORTKEY_GATEWAY_URL,
  defaultHeaders: createHeaders({
    provider: "openai",
    apiKey: "PORTKEY_API_KEY",
  }),
});

async function main() {
  const chatCompletion = await openai.chat.completions.create({
    messages: [{ role: "user", content: "Say this is a test" }],
    model: "gpt-4o",
  });

  console.log(chatCompletion.choices);
}

main();
```
```sh cURL
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "x-portkey-openai-organization: org-xxxxxxx" \
  -H "x-portkey-openai-project: proj_xxxxxxx" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-provider: openai" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user","content": "Hello!"}]
  }'
```
```python Portkey Python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="openai",
    Authorization="Bearer OPENAI_API_KEY",
    openai_organization="org-xxxxxxxxx",
    openai_project="proj_xxxxxxxxx",
)

chat_complete = portkey.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Say this is a test"}],
)

print(chat_complete.choices[0].message.content)
```
```js Portkey NodeJS
import Portkey from "portkey-ai";

const portkey = new Portkey({
  apiKey: "PORTKEY_API_KEY",
  provider: "openai",
  Authorization: "Bearer OPENAI_API_KEY",
  openaiOrganization: "org-xxxxxxxxxxx",
  openaiProject: "proj_xxxxxxxxxxxxx",
});

async function main() {
  const chatCompletion = await portkey.chat.completions.create({
    messages: [{ role: "user", content: "Say this is a test" }],
    model: "gpt-4o",
  });

  console.log(chatCompletion.choices);
}

main();
```
</CodeGroup>

</Accordion>
</AccordionGroup>
</Accordion>

### Supported Parameters

<Accordion title="List of supported & unsupported parameters from OpenAI">

| Method / Endpoint          | Supported Parameters                                                                                                                                                     |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `completions`        | model, prompt, max_tokens, temperature, top_p, n, stream, logprobs, echo, stop, presence_penalty, frequency_penalty, best_of, logit_bias, user, seed, suffix              |
| `embeddings`           | model, input, encoding_format, dimensions, user                                                                                                                           |
| `chat.completions`    | model, messages, functions, function_call, max_tokens, temperature, top_p, n, stream, stop, presence_penalty, frequency_penalty, logit_bias, user, seed, tools, tool_choice, response_format, logprobs, top_logprobs, stream_options, service_tier, parallel_tool_calls, max_completion_tokens |
| `image.generations`   | prompt, model, n, quality, response_format, size, style, user                                                                                                             |
| `create.speech`    | model, input, voice, response_format, speed                                                                                                                               |
| `create.transcription` | All parameters supported                                                                                                                                              |
| `create.translation`   | All parameters supported                                                                                                                                              |

</Accordion>

### Supported Models

<Accordion title="List of OpenAI models supported by Portkey">
</Accordion>

### Supported Endpoints & Capabilities

<Accordion title="List of OpenAI endpoints supported by Portkey">
</Accordion>

### Limitations

<Warning>
Portkey does not support the following OpenAI features:
- Streaming for audio endpoints
</Warning>
