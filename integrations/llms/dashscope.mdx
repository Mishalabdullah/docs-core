---
title: 'Dashscope'
description: 'Integrate Dashscope with Portkey for seamless completions, prompt management, and advanced features like streaming, function calling, and fine-tuning.'
---

<Note>
**Portkey Provider Slug:** `dashscope`
</Note>

## Overview

Portkey offers native integrations with [dashscope](https://dashscope.aliyun.com/) for Node.js, Python, and REST APIs. By combining Portkey with Dashscope, you can create production-grade AI applications with enhanced reliability, observability, and advanced features.

<CardGroup cols={1}>
  <Card title="Dashscope Documentation" icon="book" href="https://help.aliyun.com/zh/model-studio/developer-reference/compatibility-of-openai-with-dashscope">
    Explore the official Dashscope documentation for comprehensive details on their APIs and models.
  </Card>
</CardGroup>

## Getting Started

<Steps>
  <Step title="Obtain your Dashscope API Key">
    Visit the [Dashscope dashboard](https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key) to generate your API key.
  </Step>

  <Step title="Create a Virtual Key in Portkey">
    Portkey's virtual key vault simplifies your interaction with Dashscope. Virtual keys act as secure aliases for your actual API keys, offering enhanced security and easier management through [budget limits](/product/ai-gateway/virtual-keys/budget-limits) to control your API usage.

    Use the Portkey app to create a [virtual key](/product/ai-gateway/virtual-keys) associated with your Dashscope API key.
  </Step>

  <Step title="Initialize the Portkey Client">
    Now that you have your virtual key, set up the Portkey client:

    ### Portkey Hosted App
    Use the Portkey API key and the Dashscope virtual key to initialize the client in your preferred programming language.

    <CodeGroup>
    ```python Python
    from portkey_ai import Portkey

    portkey = Portkey(
        api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
        virtual_key="VIRTUAL_KEY"   # Replace with your virtual key for Dashscope
    )
    ```

    ```javascript Node.js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
        apiKey: "PORTKEY_API_KEY", // defaults to process.env["PORTKEY_API_KEY"]
        virtualKey: "VIRTUAL_KEY" // Your Dashscope Virtual Key
    })
    ```
    </CodeGroup>

    ### Open Source Use
    Alternatively, use Portkey's Open Source AI Gateway to enhance your app's reliability with minimal code:

    <CodeGroup>
    ```python Python
    from portkey_ai import Portkey, PORTKEY_GATEWAY_URL

    portkey = Portkey(
        api_key="dummy",  # Replace with your Portkey API key
        base_url=PORTKEY_GATEWAY_URL,
        Authorization="DASHSCOPE_API_KEY", # Replace with your Dashscope API Key
        provider="dashscope"
    )
    ```

    ```javascript Node.js
    import Portkey, { PORTKEY_GATEWAY_URL } from 'portkey-ai'

    const portkey = new Portkey({
        apiKey: "dummy", // Replace with your Portkey API key
        baseUrl: PORTKEY_GATEWAY_URL,
        Authorization: "DASHSCOPE_API_KEY", // Replace with your Dashscope API Key
        provider: "dashscope"
    })
    ```
    </CodeGroup>
  </Step>
</Steps>

ðŸ”¥ That's it! You've integrated Portkey into your application with just a few lines of code. Now let's explore making requests using the Portkey client.

## Supported Models

<Accordion title="Supported Dashscope Models">

`Chat` - qwen-long, qwen-max, qwen-max-0428, qwen-max-0403, qwen-max-0107, qwen-plus, qwen-plus-0806, qwen-plus-0723, qwen-plus-0624, qwen-plus-0206, qwen-turbo, qwen-turbo-0624, qwen-turbo-0206, qwen2-57b-a14b-instruct, qwen2-72b-instruct, qwen2-7b-instruct, qwen2-1.5b-instruct, qwen2-0.5b-instruct, qwen1.5-110b-chat, qwen1.5-72b-chat, qwen1.5-32b-chat, qwen1.5-14b-chat, qwen1.5-7b-chat, qwen1.5-1.8b-chat, qwen1.5-0.5b-chat, codeqwen1.5-7b-chat, qwen-72b-chat, qwen-14b-chat, qwen-7b-chat, qwen-1.8b-longcontext-chat, qwen-1.8b-chat, qwen2-math-72b-instruct, qwen2-math-7b-instruct, qwen2-math-1.5b-instruct


`Embedding`- text-embedding-v1, text-embedding-v2, text-embedding-v3

</Accordion>


## Supported Endpoints and Parameters

| Endpoint          | Supported Parameters                                                                                                                                                     |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `chatComplete`    | messages, max_tokens, temperature, top_p, stream, presence_penalty, frequency_penalty   |
| `embed`           | model, input, encoding_format, dimensions, user                                                                                                                           |



## Dashscope Supported Features

### Chat Completions

Generate chat completions using Dashscope models through Portkey:

<CodeGroup>
```python Python
completion = portkey.chat.completions.create(
    messages=[{"role": "user", "content": "Say this is a test"}],
    model="qwen-turbo"
)

print(completion.choices[0].message.content)
```

```javascript Node.js
const chatCompletion = await portkey.chat.completions.create({
    messages: [{ role: 'user', content: 'Say this is a test' }],
    model: 'qwen-turbo',
});

console.log(chatCompletion.choices[0].message.content);
```

```curl REST
curl -X POST "https://api.portkey.ai/v1/chat/completions" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -d '{
       "messages": [{"role": "user", "content": "Say this is a test"}],
       "model": "qwen-turbo"
     }'
```
</CodeGroup>

### Embeddings

Generate embeddings for text using Dashscope embedding models:

<CodeGroup>
```python Python
response = portkey.embeddings.create(
    input="Your text string goes here",
    model="text-embedding-v1"
)

print(response.data[0].embedding)
```

```javascript Node.js
const response = await portkey.embeddings.create({
  input: "Your text string goes here",
  model: "text-embedding-v1"
});

console.log(response.data[0].embedding);
```

```curl REST
curl -X POST "https://api.portkey.ai/v1/embeddings" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -d '{
       "input": "Your text string goes here",
       "model": "text-embedding-v1"
     }'
```
</CodeGroup>





# Portkey's Advanced Features

## Track End-User IDs

Portkey allows you to track user IDs passed with the user parameter in Dashscope requests, enabling you to monitor user-level costs, requests, and more:

<CodeGroup>
```python Python
response = portkey.chat.completions.create(
  model="qwen-turbo",
  messages=[{"role": "user", "content": "Say this is a test"}],
  user="user_123456"
)
```

```javascript Node.js
const chatCompletion = await portkey.chat.completions.create({
  messages: [{ role: "user", content: "Say this is a test" }],
  model: "qwen-turbo",
  user: "user_12345",
});
```

```curl REST
curl -X POST "https://api.portkey.ai/v1/chat/completions" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_PORTKEY_API_KEY" \
     -d '{
       "model": "qwen-turbo",
       "messages": [{"role": "user", "content": "Say this is a test"}],
       "user": "user_123456"
     }'
```
</CodeGroup>

When you include the user parameter in your requests, Portkey logs will display the associated user ID, as shown in the image below:

<img src="/images/llms/logs.png" alt="Portkey Logs with User ID" />

In addition to the `user` parameter, Portkey allows you to send arbitrary custom metadata with your requests. This powerful feature enables you to associate additional context or information with each request, which can be useful for analysis, debugging, or other custom use cases.

<CardGroup cols={1}>
  <Card title="Learn More About Metadata" icon="tags" href="/product/observability/metadata">
    Explore how to use custom metadata to enhance your request tracking and analysis.
  </Card>
</CardGroup>

## Using The Gateway Config

Here's a simplified version of how to use Portkey's Gateway Configuration:

<Steps>
  <Step title="Create a Gateway Configuration" titleSize="h3">
    You can create a Gateway configuration using the Portkey Config Dashboard or by writing a JSON configuration in your code. In this example, requests are routed based on the user's subscription plan (paid or free).

    ```json
    config = {
      "strategy": {
        "mode": "conditional",
        "conditions": [
          {
            "query": { "metadata.user_plan": { "$eq": "paid" } },
            "then": "qwen-turbo"
          },
          {
            "query": { "metadata.user_plan": { "$eq": "free" } },
            "then": "gpt-3.5"
          }
        ],
        "default": "base-gpt4"
      },
      "targets": [
        {
          "name": "qwen-turbo",
          "virtual_key": "xx"
        },
        {
          "name": "gpt-3.5",
          "virtual_key": "yy"
        }
      ]
    }
    ```
  </Step>

  <Step title="Process Requests" titleSize="h3">
    When a user makes a request, it will pass through Portkey's AI Gateway. Based on the configuration, the Gateway routes the request according to the user's metadata.
    <img src="/images/llms/conditional-routing.png" alt="Conditional Routing Diagram" />
  </Step>

  <Step title="Set Up the Portkey Client" titleSize="h3">
    Pass the Gateway configuration to your Portkey client. You can either use the config object or the Config ID from Portkey's hosted version.

    <CodeGroup>
    ```python Python
    from portkey_ai import Portkey

    portkey = Portkey(
        api_key="PORTKEY_API_KEY",
        virtual_key="VIRTUAL_KEY",
        config=portkey_config
    )
    ```

    ```javascript Node.js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
      apiKey: "PORTKEY_API_KEY",
      virtualKey: "VIRTUAL_KEY",
      config: portkeyConfig
    })
    ```
    </CodeGroup>
  </Step>
</Steps>

That's it! Portkey seamlessly allows you to make your AI app more robust using built-in gateway features. Learn more about advanced gateway features:

<CardGroup cols={2}>
  <Card title="Load Balancing" icon="link" href="/product/ai-gateway/load-balancing">
    Distribute requests across multiple targets based on defined weights.
  </Card>
  <Card title="Fallbacks" icon="life-ring" href="/product/ai-gateway/fallbacks">
    Automatically switch to backup targets if the primary target fails.
  </Card>
  <Card title="Conditional Routing" icon="route" href="/product/ai-gateway/conditional-routing">
    Route requests to different targets based on specified conditions.
  </Card>
  <Card title="Caching" icon="database" href="/product/ai-gateway/cache-simple-and-semantic">
    Enable caching of responses to improve performance and reduce costs.
  </Card>
</CardGroup>

## Guardrails

Portkey's AI gateway enables you to enforce input/output checks on requests by applying custom hooks before and after processing. Protect your user's/company's data by using PII guardrails and many more available on Portkey Guardrails:

```json
{
	"virtual_key":"dashscope-xxx",
	"before_request_hooks": [{
		"id": "input-guardrail-id-xx"
	}],
	"after_request_hooks": [{
		"id": "output-guardrail-id-xx"
	}]
}
```

<Card title="Learn More About Guardrails" icon="shield-check" href="/product/guardrails">
  Explore Portkey's guardrail features to enhance the security and reliability of your AI applications.
</Card>

## Next Steps

The complete list of features supported in the SDK are available in our comprehensive documentation:

<Card title="Portkey SDK Documentation" icon="book-open" href="/api-reference/portkey-sdk-client">
  Explore the full capabilities of the Portkey SDK and how to leverage them in your projects.
</Card>

---



For the most up-to-date information on supported features and endpoints, please refer to our [API Reference](/docs/api-reference/introduction).
